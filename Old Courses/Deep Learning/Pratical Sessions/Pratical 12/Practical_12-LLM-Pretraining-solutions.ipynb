{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emia6qd0MdJD"
      },
      "source": [
        "# LLM pretraining\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owbKvxwhcMuO"
      },
      "source": [
        "In this notebook, we will explore the process of pretraining a large language model (LLM) from scratch, focusing on a decoder-only transformer architecture.\n",
        "\n",
        "The vast majority of current LLMs are trained with a causal auto-regressive objective, where the model predicts the next token given the previous tokens.\n",
        "\n",
        "We will follow this approach to train a character-level LLM on the works of Shakespeare."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dataclasses\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.cuda\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import time\n",
        "\n",
        "\n",
        "from typing import Optional"
      ],
      "metadata": {
        "id": "torZ8wDB_eGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhmbmir2cuCn"
      },
      "source": [
        "## Part I: The training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7htNP9Jc0z9"
      },
      "source": [
        "We will start by preparing the data to train the model, available at [https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        ").\n",
        "\n",
        "This data was originally used in the following [blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/), also worth reading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiy0tKQfMvUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d721bda-b8e6-4b9d-dfc2-3ec4601d7e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-04 09:42:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-01-04 09:42:30 (23.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "Length in characters:  1115394\n",
            "First 100 characters --------------------------------------------------\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "# Downloading the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Length in characters: \", len(text))\n",
        "\n",
        "# let's look at the first 100 characters\n",
        "print(\"First 100 characters\", \"-\" * 50)\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dKUimzLevzp"
      },
      "source": [
        "From here, we can create the model vocabulary, which maps the characters into token ids.\n",
        "\n",
        "We can train the vocabulary on the whole training set and perform some light testing to check that everything is working as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6E7HG-YNigp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b6b40c-c6a7-4a26-d6f5-982094de7251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary[size=65]{'w': 0, '&': 1, 'r': 2, 'Q': 3, '?': 4, 'Z': 5, 'I': 6, 'j': 7, '.': 8, 'N': 9, 'g': 10, 'V': 11, 'k': 12, 'c': 13, 'a': 14, 'd': 15, ',': 16, '$': 17, '!': 18, 'z': 19, 'F': 20, '\\n': 21, ';': 22, 'u': 23, 'D': 24, 'e': 25, 'H': 26, 'K': 27, 's': 28, '3': 29, 'm': 30, 'O': 31, 'A': 32, 'U': 33, 'n': 34, 'C': 35, 'B': 36, 'R': 37, 'T': 38, 'q': 39, 'J': 40, 'x': 41, 'G': 42, 'l': 43, 'L': 44, 'b': 45, ':': 46, 'Y': 47, 'i': 48, 'y': 49, 'o': 50, 'P': 51, 'f': 52, 'v': 53, 'X': 54, ' ': 55, 'p': 56, '-': 57, 'E': 58, 'S': 59, 'W': 60, 'h': 61, 'M': 62, \"'\": 63, 't': 64}\n"
          ]
        }
      ],
      "source": [
        "class Vocabulary:\n",
        "  \"\"\"Character-level vocabulary\"\"\"\n",
        "\n",
        "  def __init__(self, text: str):\n",
        "    \"\"\"Initialize the vocabulary, mapping individual characters to integers.\"\"\"\n",
        "    # We create two dictionaries, ctoi and itoc: the former mapping characters\n",
        "    # to unique integers and the latter with the reverse mapping.\n",
        "    chars = set(text)\n",
        "    self.ctoi = {c: i for i, c in enumerate(chars)}\n",
        "    self.itoc = {v: k for k, v in self.ctoi.items()}\n",
        "\n",
        "  def encode(self, text: str) -> list[int]:\n",
        "    \"\"\"Encode a string as a list of token ids.\"\"\"\n",
        "    return [self.ctoi[t] for t in text]\n",
        "\n",
        "  def decode(self, ids: list[int]) -> str:\n",
        "    \"\"\"Decode a list of token ids into a string.\"\"\"\n",
        "    return \"\".join(self.itoc[i] for i in ids)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ctoi)\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Vocabulary[size={len(self)}]{self.ctoi}\"\n",
        "\n",
        "vocab = Vocabulary(text)\n",
        "print(vocab)\n",
        "\n",
        "# Test if encoding/decoding work\n",
        "first_chars = text[:1000]\n",
        "encoded = vocab.encode(first_chars)\n",
        "decoded = vocab.decode(encoded)\n",
        "assert first_chars == decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCWHEgmgfpEg"
      },
      "source": [
        "With our vocabulary, we can encode the full text data as a tensor and split it into data for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfmYqseQfoxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86215e65-5b2d-49ee-a95a-d929461aa0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 1115394\n",
            "Train tokens: 1113394\n",
            "Val tokens:  2000\n"
          ]
        }
      ],
      "source": [
        "# Encode all text\n",
        "encoded_text = torch.tensor(vocab.encode(text))\n",
        "print(\"Total tokens:\", encoded_text.size(0))\n",
        "# Split into train and validation\n",
        "val_size = 2000\n",
        "\n",
        "encoded_train = encoded_text[:-val_size]\n",
        "encoded_val = encoded_text[-val_size:]\n",
        "print(\"Train tokens:\", encoded_train.size(0))\n",
        "print(\"Val tokens: \", encoded_val.size(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akQ2Ugf0gb66"
      },
      "source": [
        "From here, we can create the dataset class to iterate the data. Each record will be a chunk of adjacent tokens and the model will learn to predict the next token from the previous ones.\n",
        "\n",
        "We will train on sequences of 512 tokens, which offers a good trade-off between previous context length and efficiency (due to the quadractic behavior of attention)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OYYZCc9bj3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f7d306-41d3-4b7f-9b0c-44baee56a2da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 1112881 , val size: 1487\n"
          ]
        }
      ],
      "source": [
        "# Predict on block_size contiguous tokens\n",
        "seq_len = 512\n",
        "\n",
        "\n",
        "class ShakespeareDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"Dataset class return input and target ids for a given idx\"\"\"\n",
        "\n",
        "  def __init__(self, data: torch.Tensor, seq_len: int):\n",
        "    self.data = data\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "\n",
        "  def __getitem__(self, i: int) -> torch.Tensor:\n",
        "    \"\"\"Selects the seq_len+1 tokens starting at index i for training.\n",
        "\n",
        "    Returns:\n",
        "        (x, y) tuple, with the input and target ids.\n",
        "    \"\"\"\n",
        "    # Account for extra one in LM offset\n",
        "    data = self.data[i:i + self.seq_len + 1]\n",
        "    x = data[:-1]\n",
        "    y = data[1:]\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"The length of the dataset, only considering complete sequences.\"\"\"\n",
        "    return len(self.data) - self.seq_len - 1\n",
        "\n",
        "train = ShakespeareDataset(encoded_train, seq_len)\n",
        "val = ShakespeareDataset(encoded_val, seq_len)\n",
        "print(\"Train size:\", len(train), \", val size:\", len(val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOaZZGCICviC"
      },
      "source": [
        "## Part II: The Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB7YL90ctBwg"
      },
      "source": [
        "In this section of the notebook, we will implement a standard decoder-only transformer model.\n",
        "\n",
        "There are many flavours of the transformer architecture, varying all components of the transformer network. In this notebook, we will consider the following configuration:\n",
        "\n",
        "* **Learnt Positional Encoddings** - In conjuntion with sinosoidal embeddings, these are one of the first approaches for encoding positional information in the transformer architecture, by learning position vectors to encode this information. Currently, however, most transformers adopt [RoPE embeddings](https://arxiv.org/abs/2104.09864).\n",
        "\n",
        "* **Pre layer normalization** - In contrast with the original transformer, we will add layer normalization *before* the attention and mlp blocks, which is shown to improve performance. Nevertheless, we will adopt the original layer normalization instead of the, nowadays more common, [RMS normalization](https://arxiv.org/abs/1910.07467).\n",
        "\n",
        "* **MLP with gelu activation** - Following the original transformer, we adopt a two layer MLP, but replace the ReLU activation with [GeLU](https://arxiv.org/abs/1606.08415). Recently, larger models also successfuly adopted variations of [GLUs](https://arxiv.org/abs/2002.05202).\n",
        "\n",
        "We start by defining the model configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NLfTPutY5ml"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "  seq_len: int = seq_len\n",
        "  n_layers: int = 6\n",
        "  n_heads: int = 6\n",
        "  hidden_size: int = 384\n",
        "  # Using the ffn hidden size as 4 times the\n",
        "  # network hidden size is a standard value.\n",
        "  ffn_hidden_size: int = 384 * 4\n",
        "  # Using a head size of hidden_size // n_heads\n",
        "  # is also a standard approach.\n",
        "  head_size: int = 384 // 6\n",
        "  vocab_size: int = len(vocab)\n",
        "  dropout = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCuB0WUYuHYd"
      },
      "source": [
        "Afterwards, we will define the self-attention mechanism. In order to make the implementation more efficient, we will:\n",
        "\n",
        "* Sharing projection matrices across heads;\n",
        "* Using torch.nn.functional.scaled_dot_product_attention for an efficient implementation of scaled dot-product attention. This implementation is based on [FlashAttention2](https://arxiv.org/abs/2307.08691)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg-wikzjuDQr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, config: Config, /, *, flash: bool = True):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.flash = True\n",
        "    # As is common in most transformer implementations, we will have single projection\n",
        "    # matrices to compute queries, keys and values for all heads.\n",
        "    # As such, each projection matrix will map an input of hidden_size to an output\n",
        "    # of n_heads * head_size.\n",
        "    self.queries_proj = nn.Linear(config.hidden_size, config.n_heads * config.head_size)\n",
        "    self.keys_proj = nn.Linear(config.hidden_size, config.n_heads * config.head_size)\n",
        "    self.values_proj = nn.Linear(config.hidden_size, config.n_heads * config.head_size)\n",
        "    self.out_proj = nn.Linear(config.n_heads * config.head_size, config.hidden_size)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # x shape: (batch, seq, hidden)\n",
        "    B, T, _ = x.size()\n",
        "    n_heads, head_dim = self.config.n_heads, self.config.head_size\n",
        "\n",
        "    # Compute projections and split by queries, keys and values.\n",
        "    # q, k, v shapes: (batch, seq, n_heads x head_size).\n",
        "    q, k, v = self.queries_proj(x), self.keys_proj(x), self.values_proj(x)\n",
        "\n",
        "    # Isolate heads\n",
        "    # q, k, v shapes: (batch, n_heads, seq, head_size)\n",
        "    q = q.view(B, T, n_heads, head_dim).transpose(1, 2)\n",
        "    k = k.view(B, T, n_heads, head_dim).transpose(1, 2)\n",
        "    v = v.view(B, T, n_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "    # Compute scaled dot product attention:\n",
        "    # Hint 1: Use torch.nn.functional.scaled_dot_product_attention.\n",
        "    # Hint 2: Be careful to only add dropout during training and ensure a causal mask.\n",
        "    # out shape: (batch, n_heads, seq, head_size)\n",
        "    dropout = self.config.dropout if self.training else 0.0\n",
        "    if self.flash:\n",
        "      out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout, is_causal=True)\n",
        "    else:\n",
        "      scores = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "      causal_mask = torch.tril(torch.ones(T, T))\n",
        "      causal_mask = causal_mask.view(1, 1, T, T)\n",
        "      scores = scores.masked_fill(causal_mask == 0, float(\"-inf\"))\n",
        "      attn = F.softmax(scores, dim=-1)\n",
        "      attn = F.dropout(attn, p=dropout)\n",
        "      out = attn @ v\n",
        "\n",
        "    # Reorder to have all head values for token together\n",
        "    # out shape: (batch, seq, n_heads * head_size)\n",
        "    out = out.transpose(1, 2).contiguous().view(B, T, -1)\n",
        "\n",
        "    # Compute the final projection\n",
        "    # out shape: (batch, seq, hidden_size)\n",
        "    out = self.out_proj(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we will define the MLP layer of the transformer. In our implementation, we will adopt:\n",
        "* Up projection layer mapping hidden_size to ffn_hidden_size;\n",
        "* Activation function (GeLU);\n",
        "* Down projection layer mapping ffn_hidden_size to hidden_size;\n",
        "* Dropout."
      ],
      "metadata": {
        "id": "b7sLab5DepDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    self.up_proj = nn.Linear(config.hidden_size, config.ffn_hidden_size)\n",
        "    self.act = nn.GELU()\n",
        "    self.down_proj = nn.Linear(config.ffn_hidden_size, config.hidden_size)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.up_proj(x)\n",
        "    x = self.act(x)\n",
        "    x = self.down_proj(x)\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "gmJMINI_eoNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lGdWenOxAlz"
      },
      "source": [
        "Now, we will define the transformer block. In contrast with the original transformer implementation, we will apply layer normalization before MHA and the MLP."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    self.attn = MultiHeadAttention(config)\n",
        "    self.attn_norm = nn.LayerNorm(config.hidden_size)\n",
        "    self.mlp = MLP(config)\n",
        "    self.mlp_norm = nn.LayerNorm(config.hidden_size)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    h = self.attn_norm(x)\n",
        "    h = self.attn(h)\n",
        "    x = x + h\n",
        "    h = self.mlp_norm(x)\n",
        "    h = self.mlp(h)\n",
        "    x = x + h\n",
        "    return x"
      ],
      "metadata": {
        "id": "G-jXu95piwON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will define the transformer decoder, using a causal language modeling objective. For this example, we use learnt embeddings to represent positional information in the transformer."
      ],
      "metadata": {
        "id": "iJM-iVehizyA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SagYJ1_XsQ75"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "    # Initialize both embedding layers, one with the size of the vocabulary,\n",
        "    # and another with the size of the sequence length.\n",
        "    self.tok_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "    self.pos_emb = nn.Embedding(config.seq_len, config.hidden_size)\n",
        "    # Initialize the transformer blocks\n",
        "    self.blocks = nn.ModuleList(Block(config) for _ in range(config.n_layers))\n",
        "    # It is also common to add a final layer norm before the lm_head\n",
        "    self.final_norm = nn.LayerNorm(config.hidden_size)\n",
        "    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      inputs: torch.Tensor,\n",
        "      *,\n",
        "      labels: Optional[torch.Tensor] = None\n",
        "  ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "    \"\"\"Computes the forward of a transformer model.\n",
        "\n",
        "    Returns a tuple with two elements. The first is the final logits,\n",
        "    and the second is the loss (only when labels are provided)\n",
        "    \"\"\"\n",
        "    tok_emb = self.tok_emb(inputs)\n",
        "    pos_emb = self._get_pos_embeddings(inputs)\n",
        "\n",
        "    hidden_states = tok_emb + pos_emb\n",
        "\n",
        "    for block in self.blocks:\n",
        "      hidden_states = block(hidden_states)\n",
        "\n",
        "    hidden_states = self.final_norm(hidden_states)\n",
        "    logits = self.lm_head(hidden_states)\n",
        "\n",
        "    if labels is None:\n",
        "      return logits, None\n",
        "\n",
        "    logits = logits.view(-1, logits.size(-1))\n",
        "    labels = labels.view(-1)\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "    return logits, loss\n",
        "\n",
        "  def _get_pos_embeddings(self, tokens: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Computes the positional embeddings for the given tokens.\"\"\"\n",
        "    batch_size, seq_len = tokens.size()\n",
        "    pos_idx = torch.arange(seq_len, device=tokens.device)\n",
        "    pos_emb = self.pos_emb(pos_idx)\n",
        "    return pos_emb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfOy4OWJxPGa"
      },
      "source": [
        "## Part III: Training and Evaluation\n",
        "\n",
        "In the final part of this notebook, we will implement the training loop, as well as a generation function, and train our transformer.\n",
        "\n",
        "In order to mitigate expensive cost of training language models, we will implement [mixed precision training](https://arxiv.org/abs/1710.03740). In this approach, while the weights are stored in fp32, the operations inside the model are performed with fp16, levergaging faster gpu implementations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(\n",
        "    *,\n",
        "    model: Transformer,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scaler: torch.amp.GradScaler,\n",
        "    device: str,\n",
        "    x: torch.Tensor,\n",
        "    y: torch.Tensor,\n",
        "):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Runs the model in mixed precision, which improves overall speed\n",
        "  # with minimal performance degradation.\n",
        "  # https://pytorch.org/docs/stable/notes/amp_examples.html#typical-mixed-precision-training\n",
        "  with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "    _, loss = model(x, labels=y)\n",
        "\n",
        "  scaler.scale(loss).backward()\n",
        "\n",
        "  scaler.step(optimizer)\n",
        "\n",
        "  scaler.update()\n",
        "\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "HcLQcMGaDP4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlwQIoSuVOln"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad\n",
        "def validate(\n",
        "    *,\n",
        "    model: Transformer,\n",
        "    val_dataloader: torch.utils.data.DataLoader,\n",
        "    device: str,\n",
        "):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    for x, y in val_dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "          _, loss = model(x, labels=y)\n",
        "        total_loss += loss\n",
        "\n",
        "    total_loss /= len(val_dataloader)\n",
        "\n",
        "    model.train()\n",
        "    return total_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARtsnw91YoeX"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad\n",
        "def generate(\n",
        "  model: nn.Module,\n",
        "  vocab: Vocabulary,\n",
        "  input: str,\n",
        "  new_tokens: int,\n",
        "  device,\n",
        "):\n",
        "  model.eval()\n",
        "  idx = vocab.encode(input)\n",
        "  idx = torch.tensor(idx, dtype=torch.long, device=device)\n",
        "  idx = idx.unsqueeze(0)\n",
        "\n",
        "  for t in range(new_tokens):\n",
        "    # The model only considers the previous seq len characters.\n",
        "    ctx = idx[:, -seq_len:]\n",
        "\n",
        "    with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "      logits, _ = model(ctx)\n",
        "\n",
        "    next_logits = logits[:, -1, :]\n",
        "\n",
        "    next_probs = torch.softmax(next_logits, dim=-1)\n",
        "\n",
        "    next_idx = torch.multinomial(next_probs, num_samples=1)\n",
        "    # append sampled index to the running sequence and continue\n",
        "    idx = torch.cat((idx, next_idx), dim=1)\n",
        "\n",
        "  idx = idx.tolist()[0]\n",
        "  output = vocab.decode(idx)\n",
        "  model.train()\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation(\n",
        "    *,\n",
        "    model,\n",
        "    val_dataloader,\n",
        "    device,\n",
        "    val_gen_tokens=100):\n",
        "  val_input = \"\\n\" # Simulate the end of the last paragraph\n",
        "\n",
        "  val_loss = validate(\n",
        "      model=model,\n",
        "      val_dataloader=val_dataloader,\n",
        "      device=device,\n",
        "  )\n",
        "  print(\"Validation\", \"-\" * 39)\n",
        "  print(f\"Total loss={val_loss:.3f}\")\n",
        "  test_input = \"\\n\" # Continue to use new line as if beginning a new paragraph\n",
        "  test_gen_len = 200\n",
        "  test_output = generate(\n",
        "      model,\n",
        "      vocab,\n",
        "      test_input,\n",
        "      test_gen_len,\n",
        "      device\n",
        "  )\n",
        "  print(test_output)\n",
        "  print(\"-\" * 50)\n",
        "  return val_loss"
      ],
      "metadata": {
        "id": "ZkwRanJ1JN-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed = 42):\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.set_float32_matmul_precision(\"high\")"
      ],
      "metadata": {
        "id": "Ro7TCXQYJzym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2caBQniwDqq"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    *,\n",
        "    config = Config(),\n",
        "    num_steps=2000,\n",
        "    val_every=500,\n",
        "    log_every=50):\n",
        "\n",
        "  set_seed()\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device == \"cpu\":\n",
        "    print(\"WARNING: Using cpu device, training will be slow\")\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(\n",
        "      train, batch_size=64, shuffle=True, pin_memory=True, num_workers=2, prefetch_factor=2,\n",
        "  )\n",
        "  val_dataloader = torch.utils.data.DataLoader(\n",
        "      val, batch_size=64, shuffle=False, pin_memory=True, num_workers=2, prefetch_factor=2,\n",
        "  )\n",
        "\n",
        "  model = Transformer(config)\n",
        "\n",
        "  model_params = sum(p.numel() for p in model.parameters())\n",
        "  print(f\"Total parameters: {model_params / 1e6:.2f}M\")\n",
        "\n",
        "  optimizer = torch.optim.AdamW(\n",
        "      model.parameters(), lr=1e-3, weight_decay=0.1, betas=(0.9, 0.95), fused=True\n",
        "  )\n",
        "  scaler = torch.amp.GradScaler()\n",
        "  model.to(device)\n",
        "\n",
        "  dataiter = iter(train_dataloader)\n",
        "  start_time = time.time()\n",
        "  start_step = time.time()\n",
        "\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  val_losses.append(run_validation(\n",
        "      model=model,\n",
        "      val_dataloader=val_dataloader,\n",
        "      device=device,\n",
        "  ))\n",
        "\n",
        "  for step in range(1, num_steps + 1):\n",
        "    x, y = next(dataiter)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    token_count = x.numel()\n",
        "    loss = train_step(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        scaler=scaler,\n",
        "        device=device,\n",
        "        x=x,\n",
        "        y=y)\n",
        "\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    step_time = time.time() - start_step\n",
        "    start_step = time.time()\n",
        "    if step % log_every == 0:\n",
        "      completed = step / num_steps * 100\n",
        "      ellapsed = time.time() - start_time\n",
        "      tok_per_sec = token_count / step_time\n",
        "      fmt_ellapsed = time.strftime(\"%H:%M:%S\", time.gmtime(ellapsed))\n",
        "      print(\n",
        "          f\"Step {step}/{num_steps} ({completed:.0f}%): \"\n",
        "          f\"TrainTime={fmt_ellapsed}, \"\n",
        "          f\"Loss={loss:.3f}, \"\n",
        "          f\"StepTime={step_time:.2}s\"\n",
        "      )\n",
        "    if step % val_every == 0:\n",
        "      val_losses.append(run_validation(\n",
        "          model=model,\n",
        "          val_dataloader=val_dataloader,\n",
        "          device=device\n",
        "      ))\n",
        "\n",
        "  test_input = \"\\n\" # Continue to use new line as if beginning a new paragraph\n",
        "  test_gen_len = 1000\n",
        "  test_output = generate(\n",
        "      model,\n",
        "      vocab,\n",
        "      test_input,\n",
        "      test_gen_len,\n",
        "      device\n",
        "  )\n",
        "  print(\"Final generation\", \"-\" * 30)\n",
        "  print(test_output)\n",
        "\n",
        "  return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeOkj3UYgf87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3056c1-a0a5-4254-a2a7-c552ba1bd089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 10.89M\n",
            "Validation ---------------------------------------\n",
            "Total loss=4.318\n",
            "\n",
            "G!NF&HdVmfBJH,yZCfAjrjqZSAIUGGNmqmH ,dyIsXgMAz;ISSFR\n",
            " uPB$sjecONMuomXKhYod.e\n",
            "h\n",
            "AsQOvPbbmXshcHRkS.Cz\n",
            "xv$'VI3ulA;YtmGQs$?nFP!fH$d!YyiHX-\n",
            "J?:Xxq?PUqX,F.wSIoTiKe!bX-x!EYPgxAuVWjF-deYiH!H!YAAhjYOJXSKv$:Yxq\n",
            "--------------------------------------------------\n",
            "Step 50/2000 (2%): TrainTime=00:00:16, Loss=2.477, StepTime=0.22s\n",
            "Step 100/2000 (5%): TrainTime=00:00:27, Loss=2.480, StepTime=0.22s\n",
            "Step 150/2000 (8%): TrainTime=00:00:38, Loss=2.412, StepTime=0.22s\n",
            "Step 200/2000 (10%): TrainTime=00:00:49, Loss=2.330, StepTime=0.23s\n",
            "Step 250/2000 (12%): TrainTime=00:01:01, Loss=2.194, StepTime=0.23s\n",
            "Step 300/2000 (15%): TrainTime=00:01:13, Loss=2.047, StepTime=0.24s\n",
            "Step 350/2000 (18%): TrainTime=00:01:25, Loss=1.925, StepTime=0.24s\n",
            "Step 400/2000 (20%): TrainTime=00:01:36, Loss=1.839, StepTime=0.24s\n",
            "Step 450/2000 (22%): TrainTime=00:01:48, Loss=1.747, StepTime=0.23s\n",
            "Step 500/2000 (25%): TrainTime=00:02:00, Loss=1.661, StepTime=0.23s\n",
            "Validation ---------------------------------------\n",
            "Total loss=1.642\n",
            "\n",
            "Bost Of or but fir the tee with all theart of me,\n",
            "And which of boonain bly Pralinces;\n",
            "O prover Of it then that young be king?\n",
            "\n",
            "First Server:\n",
            "Thy know in Challan of Me, in dosth.\n",
            "\n",
            "VINCENTIO:\n",
            "Nor ball-a\n",
            "--------------------------------------------------\n",
            "Step 550/2000 (28%): TrainTime=00:02:14, Loss=1.661, StepTime=0.23s\n",
            "Step 600/2000 (30%): TrainTime=00:02:26, Loss=1.535, StepTime=0.24s\n",
            "Step 650/2000 (32%): TrainTime=00:02:37, Loss=1.573, StepTime=0.23s\n",
            "Step 700/2000 (35%): TrainTime=00:02:49, Loss=1.524, StepTime=0.23s\n",
            "Step 750/2000 (38%): TrainTime=00:03:01, Loss=1.504, StepTime=0.24s\n",
            "Step 800/2000 (40%): TrainTime=00:03:12, Loss=1.458, StepTime=0.24s\n",
            "Step 850/2000 (42%): TrainTime=00:03:24, Loss=1.437, StepTime=0.23s\n",
            "Step 900/2000 (45%): TrainTime=00:03:36, Loss=1.391, StepTime=0.23s\n",
            "Step 950/2000 (48%): TrainTime=00:03:47, Loss=1.359, StepTime=0.23s\n",
            "Step 1000/2000 (50%): TrainTime=00:03:59, Loss=1.364, StepTime=0.24s\n",
            "Validation ---------------------------------------\n",
            "Total loss=1.347\n",
            "\n",
            "What lord.\n",
            "\n",
            "KATHARINA:\n",
            "Come and thou think I love do yet on.\n",
            "\n",
            "ISABELLA:\n",
            "Comest that a mad, speak my mons reford:\n",
            "Half ye the king's wife perfs and save's gentleman:\n",
            "Our what come to think with the tru\n",
            "--------------------------------------------------\n",
            "Step 1050/2000 (52%): TrainTime=00:04:13, Loss=1.357, StepTime=0.23s\n",
            "Step 1100/2000 (55%): TrainTime=00:04:25, Loss=1.325, StepTime=0.23s\n",
            "Step 1150/2000 (57%): TrainTime=00:04:36, Loss=1.339, StepTime=0.24s\n",
            "Step 1200/2000 (60%): TrainTime=00:04:48, Loss=1.312, StepTime=0.23s\n",
            "Step 1250/2000 (62%): TrainTime=00:05:00, Loss=1.279, StepTime=0.23s\n",
            "Step 1300/2000 (65%): TrainTime=00:05:12, Loss=1.260, StepTime=0.23s\n",
            "Step 1350/2000 (68%): TrainTime=00:05:23, Loss=1.253, StepTime=0.23s\n",
            "Step 1400/2000 (70%): TrainTime=00:05:35, Loss=1.252, StepTime=0.23s\n",
            "Step 1450/2000 (72%): TrainTime=00:05:47, Loss=1.245, StepTime=0.23s\n",
            "Step 1500/2000 (75%): TrainTime=00:05:58, Loss=1.212, StepTime=0.24s\n",
            "Validation ---------------------------------------\n",
            "Total loss=1.309\n",
            "\n",
            "Now, Kate? come the queen, whom, or who lost any and blastle:\n",
            "Where is God morrow, when I not till you, by this glovern,\n",
            "and I should have heard the first face, I'll find\n",
            "'Tis no respect Tursday Stanl\n",
            "--------------------------------------------------\n",
            "Step 1550/2000 (78%): TrainTime=00:06:13, Loss=1.214, StepTime=0.24s\n",
            "Step 1600/2000 (80%): TrainTime=00:06:24, Loss=1.226, StepTime=0.23s\n",
            "Step 1650/2000 (82%): TrainTime=00:06:36, Loss=1.181, StepTime=0.23s\n",
            "Step 1700/2000 (85%): TrainTime=00:06:48, Loss=1.169, StepTime=0.23s\n",
            "Step 1750/2000 (88%): TrainTime=00:06:59, Loss=1.178, StepTime=0.23s\n",
            "Step 1800/2000 (90%): TrainTime=00:07:11, Loss=1.159, StepTime=0.23s\n",
            "Step 1850/2000 (92%): TrainTime=00:07:23, Loss=1.157, StepTime=0.23s\n",
            "Step 1900/2000 (95%): TrainTime=00:07:35, Loss=1.144, StepTime=0.23s\n",
            "Step 1950/2000 (98%): TrainTime=00:07:46, Loss=1.134, StepTime=0.23s\n",
            "Step 2000/2000 (100%): TrainTime=00:07:58, Loss=1.116, StepTime=0.24s\n",
            "Validation ---------------------------------------\n",
            "Total loss=1.287\n",
            "\n",
            "ISABELLA:\n",
            "Go, to the Lobback, so doth not most\n",
            "For the office of the foe.\n",
            "\n",
            "MIRANDA:\n",
            "God save thus.\n",
            "\n",
            "MIRANDA:\n",
            "Ay, my business.\n",
            "\n",
            "MIRANDA:\n",
            "I take yourself in plain to your man.\n",
            "\n",
            "LUCIO:\n",
            "I'll not buy me to\n",
            "--------------------------------------------------\n",
            "Final generation ------------------------------\n",
            "\n",
            "\n",
            "MARIANA:\n",
            "You have made me\n",
            "Till not that, but he'll be more run with me.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Need!\n",
            "When she comes not bewzy.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Ay, yet it.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "LUCIO:\n",
            "I on Thursday.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I will not she frown; And please you swore,\n",
            "I will not title on my trant as sure you.\n",
            "\n",
            "LUCIO:\n",
            "I have of all of ten that of that:\n",
            "bring me that I seek for some gentleman.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "O, it's true.\n",
            "\n",
            "LUCIO:\n",
            "My wife, my liege, when I dare not thee my duke.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Farewell, I will not hear that I am gone.\n",
            "\n",
            "LUCIO:\n",
            "I would not say your wife came home; let them gentle truth.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "O, sir; sir, we are so. Look to the labon to be a hand;\n",
            "Hark, what is my wife, I promised by this?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Near, I do. By.\n",
            "\n",
            "LUCIO:\n",
            "Heaven return me you with you. passign him\n",
            "to this unfirmed? I dare you call me this;\n",
            "but have I call'd you frowned. Therefore, my\n",
            "grave the alms is possible: it bid me up for him, I\n",
            "needy when I laid, say 's.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "O, by my name, you love me to-ni\n"
          ]
        }
      ],
      "source": [
        "train_losses, val_losses = train_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create x-axis values for training and validation losses\n",
        "train_steps = list(range(len(train_losses)))\n",
        "num_val_steps = len(val_losses)\n",
        "num_steps = len(train_steps)\n",
        "val_steps = np.linspace(0, num_steps, num_val_steps, dtype=int).tolist()\n",
        "\n",
        "# Plot Cross Entropy Train losses\n",
        "plt.plot(train_steps, train_losses, label=\"Train\")\n",
        "plt.plot(val_steps, val_losses, label=\"Validation\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Cross entropy loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F-o_51xWCIgh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "d8dc4cf4-884d-47f0-86df-a3e5d5fa6c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xsPbCbsEH0IP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}