{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Sb39IOeUS6XX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKvvDAMKTjvW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def configure_seed(seed):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def load_dataset(data_path, bias=False):\n",
        "    data = np.load(data_path)\n",
        "\n",
        "    train_X = data[\"train_images\"].reshape([data[\"train_images\"].shape[0], -1])/256\n",
        "    dev_X = data[\"val_images\"].reshape([data[\"val_images\"].shape[0], -1])/256\n",
        "    test_X = data[\"test_images\"].reshape([data[\"test_images\"].shape[0], -1])/256\n",
        "\n",
        "    train_y = np.asarray(data[\"train_labels\"]).squeeze()\n",
        "    dev_y = np.asarray(data[\"val_labels\"]).squeeze()\n",
        "    test_y = np.asarray(data[\"test_labels\"]).squeeze()\n",
        "\n",
        "    if bias:\n",
        "        train_X = np.hstack((train_X, np.ones((train_X.shape[0], 1))))\n",
        "        dev_X = np.hstack((dev_X, np.ones((dev_X.shape[0], 1))))\n",
        "        test_X = np.hstack((test_X, np.ones((test_X.shape[0], 1))))\n",
        "\n",
        "    return {\n",
        "        \"train\": (train_X, train_y), \"dev\": (dev_X, dev_y), \"test\": (test_X, test_y),\n",
        "    }\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        data: the dict returned by utils.load_pneumonia_data\n",
        "        \"\"\"\n",
        "        train_X, train_y = data[\"train\"]\n",
        "        dev_X, dev_y = data[\"dev\"]\n",
        "        test_X, test_y = data[\"test\"]\n",
        "\n",
        "        self.X = torch.tensor(train_X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(train_y, dtype=torch.long)\n",
        "\n",
        "        self.dev_X = torch.tensor(dev_X, dtype=torch.float32)\n",
        "        self.dev_y = torch.tensor(dev_y, dtype=torch.long)\n",
        "\n",
        "        self.test_X = torch.tensor(test_X, dtype=torch.float32)\n",
        "        self.test_y = torch.tensor(test_y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alG5hxz7RnAk"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Deep Learning Homework 1\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4r3CylnQMB-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes, n_features, **kwargs):\n",
        "        \"\"\"\n",
        "        n_classes (int)\n",
        "        n_features (int)\n",
        "\n",
        "        The __init__ should be used to declare what kind of layers and other\n",
        "        parameters the module has. For example, a logistic regression module\n",
        "        has a weight matrix and bias vector. For an idea of how to use\n",
        "        pytorch to make weights and biases, have a look at\n",
        "        https://pytorch.org/docs/stable/nn.html\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.weights = nn.Parameter(torch.randn(n_features, n_classes) * 0.01)\n",
        "        self.bias = nn.Parameter(torch.zeros(n_classes))\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        \"\"\"\n",
        "        x (batch_size x n_features): a batch of training examples\n",
        "\n",
        "        Every subclass of nn.Module needs to have a forward() method. forward()\n",
        "        describes how the module computes the forward pass. In a log-lineear\n",
        "        model like this, for example, forward() needs to compute the logits\n",
        "        y = Wx + b, and return y (you don't need to worry about taking the\n",
        "        softmax of y because nn.CrossEntropyLoss does that for you).\n",
        "\n",
        "        One nice thing about pytorch is that you only need to define the\n",
        "        forward pass -- this is enough for it to figure out how to do the\n",
        "        backward pass.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        return x @ self.weights+ self.bias\n",
        "\n",
        "\n",
        "\n",
        "class FeedforwardNetwork(nn.Module):\n",
        "    def __init__(\n",
        "            self, n_classes, n_features, hidden_size, layers,\n",
        "            activation_type, dropout, **kwargs):\n",
        "        \"\"\"\n",
        "        n_classes (int)\n",
        "        n_features (int)\n",
        "        hidden_size (int)\n",
        "        layers (int)\n",
        "        activation_type (str)\n",
        "        dropout (float): dropout probability\n",
        "\n",
        "        As in logistic regression, the __init__ here defines a bunch of\n",
        "        attributes that each FeedforwardNetwork instance has. Note that nn\n",
        "        includes modules for several activation functions and dropout as well.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(layers):\n",
        "            if i == 0:\n",
        "                self.layers.append(nn.Linear(n_features, hidden_size))\n",
        "            else:\n",
        "                self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "\n",
        "        self.output = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if activation_type == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation_type == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        \"\"\"\n",
        "        x (batch_size x n_features): a batch of training examples\n",
        "\n",
        "        This method needs to perform all the computation needed to compute\n",
        "        the output logits from x. This will include using various hidden\n",
        "        layers, pointwise nonlinear functions, and dropout.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = self.activation(layer(x))\n",
        "            x = self.dropout(x)\n",
        "        return self.output(x)\n",
        "\n",
        "\n",
        "def train_batch(X, y, model, optimizer, criterion, **kwargs):\n",
        "    \"\"\"\n",
        "    X (n_examples x n_features)\n",
        "    y (n_examples): gold labels\n",
        "    model: a PyTorch defined model\n",
        "    optimizer: optimizer used in gradient step\n",
        "    criterion: loss function\n",
        "\n",
        "    To train a batch, the model needs to predict outputs for X, compute the\n",
        "    loss between these predictions and the \"gold\" labels y using the criterion,\n",
        "    and compute the gradient of the loss with respect to the model parameters.\n",
        "\n",
        "    Check out https://pytorch.org/docs/stable/optim.html for examples of how\n",
        "    to use an optimizer object to update the parameters.\n",
        "\n",
        "    This function should return the loss (tip: call loss.item()) to get the\n",
        "    loss as a numerical value that is not part of the computation graph.\n",
        "    \"\"\"\n",
        "\n",
        "    optimizer.zero_grad()  # Limpa gradientes acumulados de iteracoes anteriores\n",
        "    output = model(X)  #forward pass\n",
        "    loss = criterion(output, y) #loss function/cross entropy\n",
        "    loss.backward() #loss backward\n",
        "    optimizer.step()  #recalculo do peso\n",
        "    return loss.item()  #retorna a perda como um valor num√©rico\n",
        "\n",
        "\n",
        "\n",
        "def predict(model, X):\n",
        "    \"\"\"X (n_examples x n_features)\"\"\"\n",
        "    scores = model(X)  # (n_examples x n_classes)\n",
        "    predicted_labels = scores.argmax(dim=-1)  # (n_examples)\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, X, y, criterion):\n",
        "    \"\"\"\n",
        "    X (n_examples x n_features)\n",
        "    y (n_examples): gold labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    logits = model(X)\n",
        "    loss = criterion(logits, y)\n",
        "    loss = loss.item()\n",
        "    y_hat = logits.argmax(dim=-1)\n",
        "    n_correct = (y == y_hat).sum().item()\n",
        "    n_possible = float(y.shape[0])\n",
        "    model.train()\n",
        "    return loss, n_correct / n_possible\n",
        "\n",
        "\n",
        "def plot(epochs, plottables, filename=None, ylim=None):\n",
        "    \"\"\"Plot the plottables over the epochs.\n",
        "\n",
        "    Plottables is a dictionary mapping labels to lists of values.\n",
        "    \"\"\"\n",
        "    plt.clf()\n",
        "    plt.xlabel('Epoch')\n",
        "    for label, plottable in plottables.items():\n",
        "        plt.plot(epochs, plottable, label=label)\n",
        "    plt.legend()\n",
        "    if ylim:\n",
        "        plt.ylim(ylim)\n",
        "    if filename:\n",
        "        plt.savefig(filename, bbox_inches='tight')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oeSdRO4hRyfy",
        "outputId": "869f37a8-cb02-49a6-e664-3a95e7fa60d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial val acc: 0.1652\n",
            "Training epoch 1\n",
            "train loss: 1.5601 | val loss: 1.4736 | val acc: 0.4316\n",
            "Training epoch 2\n",
            "train loss: 1.3996 | val loss: 1.4046 | val acc: 0.4594\n",
            "Training epoch 3\n",
            "train loss: 1.3495 | val loss: 1.3561 | val acc: 0.4829\n",
            "Training epoch 4\n",
            "train loss: 1.3230 | val loss: 1.3557 | val acc: 0.4815\n",
            "Training epoch 5\n",
            "train loss: 1.3056 | val loss: 1.3335 | val acc: 0.4915\n",
            "Training epoch 6\n",
            "train loss: 1.2931 | val loss: 1.3191 | val acc: 0.4922\n",
            "Training epoch 7\n",
            "train loss: 1.2822 | val loss: 1.3312 | val acc: 0.4872\n",
            "Training epoch 8\n",
            "train loss: 1.2748 | val loss: 1.2988 | val acc: 0.5157\n",
            "Training epoch 9\n",
            "train loss: 1.2668 | val loss: 1.2982 | val acc: 0.5157\n",
            "Training epoch 10\n",
            "train loss: 1.2601 | val loss: 1.2888 | val acc: 0.5171\n",
            "Training epoch 11\n",
            "train loss: 1.2561 | val loss: 1.2923 | val acc: 0.5085\n",
            "Training epoch 12\n",
            "train loss: 1.2503 | val loss: 1.2830 | val acc: 0.5185\n",
            "Training epoch 13\n",
            "train loss: 1.2456 | val loss: 1.2834 | val acc: 0.5256\n",
            "Training epoch 14\n",
            "train loss: 1.2414 | val loss: 1.2792 | val acc: 0.5335\n",
            "Training epoch 15\n",
            "train loss: 1.2381 | val loss: 1.2833 | val acc: 0.5271\n",
            "Training epoch 16\n",
            "train loss: 1.2335 | val loss: 1.2867 | val acc: 0.5135\n",
            "Training epoch 17\n",
            "train loss: 1.2306 | val loss: 1.2904 | val acc: 0.5028\n",
            "Training epoch 18\n",
            "train loss: 1.2278 | val loss: 1.2843 | val acc: 0.5142\n",
            "Training epoch 19\n",
            "train loss: 1.2227 | val loss: 1.3102 | val acc: 0.4865\n",
            "Training epoch 20\n",
            "train loss: 1.2205 | val loss: 1.2904 | val acc: 0.5071\n",
            "Training epoch 21\n",
            "train loss: 1.2208 | val loss: 1.2811 | val acc: 0.5164\n",
            "Training epoch 22\n",
            "train loss: 1.2168 | val loss: 1.2819 | val acc: 0.5100\n",
            "Training epoch 23\n",
            "train loss: 1.2132 | val loss: 1.2879 | val acc: 0.5071\n",
            "Training epoch 24\n",
            "train loss: 1.2120 | val loss: 1.2659 | val acc: 0.5399\n",
            "Training epoch 25\n",
            "train loss: 1.2096 | val loss: 1.2986 | val acc: 0.5014\n",
            "Training epoch 26\n",
            "train loss: 1.2084 | val loss: 1.2813 | val acc: 0.5164\n",
            "Training epoch 27\n",
            "train loss: 1.2046 | val loss: 1.2758 | val acc: 0.5313\n",
            "Training epoch 28\n",
            "train loss: 1.2029 | val loss: 1.2860 | val acc: 0.5071\n",
            "Training epoch 29\n",
            "train loss: 1.2010 | val loss: 1.2690 | val acc: 0.5264\n",
            "Training epoch 30\n",
            "train loss: 1.2008 | val loss: 1.2748 | val acc: 0.5178\n",
            "Training epoch 31\n",
            "train loss: 1.1990 | val loss: 1.2709 | val acc: 0.5271\n",
            "Training epoch 32\n",
            "train loss: 1.1963 | val loss: 1.2642 | val acc: 0.5278\n",
            "Training epoch 33\n",
            "train loss: 1.1938 | val loss: 1.2783 | val acc: 0.5185\n",
            "Training epoch 34\n",
            "train loss: 1.1935 | val loss: 1.2651 | val acc: 0.5342\n",
            "Training epoch 35\n",
            "train loss: 1.1927 | val loss: 1.2637 | val acc: 0.5271\n",
            "Training epoch 36\n",
            "train loss: 1.1901 | val loss: 1.2714 | val acc: 0.5278\n",
            "Training epoch 37\n",
            "train loss: 1.1884 | val loss: 1.2796 | val acc: 0.5185\n",
            "Training epoch 38\n",
            "train loss: 1.1878 | val loss: 1.2705 | val acc: 0.5313\n",
            "Training epoch 39\n",
            "train loss: 1.1862 | val loss: 1.2611 | val acc: 0.5342\n",
            "Training epoch 40\n",
            "train loss: 1.1845 | val loss: 1.2619 | val acc: 0.5285\n",
            "Training epoch 41\n",
            "train loss: 1.1837 | val loss: 1.2672 | val acc: 0.5107\n",
            "Training epoch 42\n",
            "train loss: 1.1820 | val loss: 1.2576 | val acc: 0.5363\n",
            "Training epoch 43\n",
            "train loss: 1.1787 | val loss: 1.2755 | val acc: 0.5128\n",
            "Training epoch 44\n",
            "train loss: 1.1791 | val loss: 1.2686 | val acc: 0.5214\n",
            "Training epoch 45\n",
            "train loss: 1.1780 | val loss: 1.2834 | val acc: 0.5157\n",
            "Training epoch 46\n",
            "train loss: 1.1762 | val loss: 1.2999 | val acc: 0.5078\n",
            "Training epoch 47\n",
            "train loss: 1.1752 | val loss: 1.2609 | val acc: 0.5299\n",
            "Training epoch 48\n",
            "train loss: 1.1740 | val loss: 1.2608 | val acc: 0.5434\n",
            "Training epoch 49\n",
            "train loss: 1.1727 | val loss: 1.2605 | val acc: 0.5335\n",
            "Training epoch 50\n",
            "train loss: 1.1713 | val loss: 1.2571 | val acc: 0.5420\n",
            "Training epoch 51\n",
            "train loss: 1.1708 | val loss: 1.2613 | val acc: 0.5321\n",
            "Training epoch 52\n",
            "train loss: 1.1703 | val loss: 1.2542 | val acc: 0.5420\n",
            "Training epoch 53\n",
            "train loss: 1.1690 | val loss: 1.2619 | val acc: 0.5299\n",
            "Training epoch 54\n",
            "train loss: 1.1673 | val loss: 1.2675 | val acc: 0.5221\n",
            "Training epoch 55\n",
            "train loss: 1.1665 | val loss: 1.2595 | val acc: 0.5306\n",
            "Training epoch 56\n",
            "train loss: 1.1665 | val loss: 1.2597 | val acc: 0.5328\n",
            "Training epoch 57\n",
            "train loss: 1.1652 | val loss: 1.2603 | val acc: 0.5285\n",
            "Training epoch 58\n",
            "train loss: 1.1630 | val loss: 1.2586 | val acc: 0.5406\n",
            "Training epoch 59\n",
            "train loss: 1.1630 | val loss: 1.2579 | val acc: 0.5306\n",
            "Training epoch 60\n",
            "train loss: 1.1619 | val loss: 1.2637 | val acc: 0.5306\n",
            "Training epoch 61\n",
            "train loss: 1.1619 | val loss: 1.2569 | val acc: 0.5363\n",
            "Training epoch 62\n",
            "train loss: 1.1599 | val loss: 1.2711 | val acc: 0.5157\n",
            "Training epoch 63\n",
            "train loss: 1.1595 | val loss: 1.2683 | val acc: 0.5228\n",
            "Training epoch 64\n",
            "train loss: 1.1573 | val loss: 1.2626 | val acc: 0.5349\n",
            "Training epoch 65\n",
            "train loss: 1.1569 | val loss: 1.2638 | val acc: 0.5292\n",
            "Training epoch 66\n",
            "train loss: 1.1564 | val loss: 1.2739 | val acc: 0.5185\n",
            "Training epoch 67\n",
            "train loss: 1.1545 | val loss: 1.2738 | val acc: 0.5235\n",
            "Training epoch 68\n",
            "train loss: 1.1549 | val loss: 1.2666 | val acc: 0.5299\n",
            "Training epoch 69\n",
            "train loss: 1.1552 | val loss: 1.2568 | val acc: 0.5335\n",
            "Training epoch 70\n",
            "train loss: 1.1530 | val loss: 1.2656 | val acc: 0.5292\n",
            "Training epoch 71\n",
            "train loss: 1.1531 | val loss: 1.2595 | val acc: 0.5299\n",
            "Training epoch 72\n",
            "train loss: 1.1515 | val loss: 1.2628 | val acc: 0.5199\n",
            "Training epoch 73\n",
            "train loss: 1.1499 | val loss: 1.2654 | val acc: 0.5271\n",
            "Training epoch 74\n",
            "train loss: 1.1501 | val loss: 1.2536 | val acc: 0.5385\n",
            "Training epoch 75\n",
            "train loss: 1.1512 | val loss: 1.2636 | val acc: 0.5235\n",
            "Training epoch 76\n",
            "train loss: 1.1489 | val loss: 1.2625 | val acc: 0.5342\n",
            "Training epoch 77\n",
            "train loss: 1.1482 | val loss: 1.2697 | val acc: 0.5321\n",
            "Training epoch 78\n",
            "train loss: 1.1484 | val loss: 1.2603 | val acc: 0.5420\n",
            "Training epoch 79\n",
            "train loss: 1.1467 | val loss: 1.2689 | val acc: 0.5142\n",
            "Training epoch 80\n",
            "train loss: 1.1467 | val loss: 1.2604 | val acc: 0.5185\n",
            "Training epoch 81\n",
            "train loss: 1.1450 | val loss: 1.2565 | val acc: 0.5199\n",
            "Training epoch 82\n",
            "train loss: 1.1436 | val loss: 1.2559 | val acc: 0.5335\n",
            "Training epoch 83\n",
            "train loss: 1.1431 | val loss: 1.2538 | val acc: 0.5349\n",
            "Training epoch 84\n",
            "train loss: 1.1445 | val loss: 1.2581 | val acc: 0.5342\n",
            "Training epoch 85\n",
            "train loss: 1.1433 | val loss: 1.2582 | val acc: 0.5385\n",
            "Training epoch 86\n",
            "train loss: 1.1416 | val loss: 1.2746 | val acc: 0.5178\n",
            "Training epoch 87\n",
            "train loss: 1.1407 | val loss: 1.2573 | val acc: 0.5328\n",
            "Training epoch 88\n",
            "train loss: 1.1385 | val loss: 1.2560 | val acc: 0.5413\n",
            "Training epoch 89\n",
            "train loss: 1.1387 | val loss: 1.2773 | val acc: 0.5128\n",
            "Training epoch 90\n",
            "train loss: 1.1398 | val loss: 1.2586 | val acc: 0.5285\n",
            "Training epoch 91\n",
            "train loss: 1.1395 | val loss: 1.2556 | val acc: 0.5406\n",
            "Training epoch 92\n",
            "train loss: 1.1372 | val loss: 1.2645 | val acc: 0.5228\n",
            "Training epoch 93\n",
            "train loss: 1.1371 | val loss: 1.2570 | val acc: 0.5271\n",
            "Training epoch 94\n",
            "train loss: 1.1364 | val loss: 1.2705 | val acc: 0.5135\n",
            "Training epoch 95\n",
            "train loss: 1.1364 | val loss: 1.2581 | val acc: 0.5235\n",
            "Training epoch 96\n",
            "train loss: 1.1352 | val loss: 1.2604 | val acc: 0.5285\n",
            "Training epoch 97\n",
            "train loss: 1.1354 | val loss: 1.2609 | val acc: 0.5285\n",
            "Training epoch 98\n",
            "train loss: 1.1346 | val loss: 1.2575 | val acc: 0.5328\n",
            "Training epoch 99\n",
            "train loss: 1.1326 | val loss: 1.2814 | val acc: 0.5214\n",
            "Training epoch 100\n",
            "train loss: 1.1342 | val loss: 1.2626 | val acc: 0.5235\n",
            "Training took 0 minutes and 54 seconds\n",
            "Final test acc: 0.5250\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('model',\n",
        "    #                     choices=['logistic_regression', 'mlp'],\n",
        "    #                     default='logistic_regression',\n",
        "    #                     help=\"Which model should the script run?\")\n",
        "    # parser.add_argument('-epochs', default=200, type=int,\n",
        "    #                     help=\"\"\"Number of epochs to train for. You should not\n",
        "    #                     need to change this value for your plots.\"\"\")\n",
        "    # parser.add_argument('-batch_size', default=64, type=int,\n",
        "    #                     help=\"Size of training batch.\")\n",
        "    # parser.add_argument('-hidden_size', type=int, default=200)\n",
        "    # parser.add_argument('-layers', type=int, default=2)\n",
        "    # parser.add_argument('-learning_rate', type=float, default=0.002)\n",
        "    # parser.add_argument('-l2_decay', type=float, default=0.0)\n",
        "    # parser.add_argument('-dropout', type=float, default=0.3)\n",
        "    # parser.add_argument('-momentum', type=float, default=0.0)\n",
        "    # parser.add_argument('-activation',\n",
        "    #                     choices=['tanh', 'relu'], default='relu')\n",
        "    # parser.add_argument('-optimizer',\n",
        "    #                     choices=['sgd', 'adam'], default='sgd')\n",
        "    # parser.add_argument('-data_path', type=str, default='intel_landscapes.npz',)\n",
        "\n",
        "    #opt = parser.parse_args()\n",
        "    opt = {\n",
        "        'model': 'logistic_regression',\n",
        "        'epochs': 100,\n",
        "        'batch_size': 32,\n",
        "        'hidden_size': 200,\n",
        "        'layers': 2,\n",
        "        'learning_rate': 0.001,\n",
        "        'l2_decay': 0.01,\n",
        "        'dropout': 0.3,\n",
        "        'momentum': 0.0,\n",
        "        'activation': 'relu',\n",
        "        'optimizer': 'sgd',\n",
        "        'data_path': '/content/sample_data/intel_landscapes.v2.npz',\n",
        "    }\n",
        "\n",
        "    configure_seed(seed=42)\n",
        "\n",
        "    data = load_dataset(opt['data_path'])\n",
        "    dataset = ClassificationDataset(data)\n",
        "    train_dataloader = DataLoader(\n",
        "        dataset, batch_size=opt['batch_size'], shuffle=True, generator=torch.Generator().manual_seed(42))\n",
        "    dev_X, dev_y = dataset.dev_X, dataset.dev_y\n",
        "    test_X, test_y = dataset.test_X, dataset.test_y\n",
        "\n",
        "    n_classes = torch.unique(dataset.y).shape[0]  # 10\n",
        "    n_feats = dataset.X.shape[1]\n",
        "\n",
        "    # initialize the model\n",
        "    if opt['model'] == 'logistic_regression':\n",
        "        model = LogisticRegression(n_classes, n_feats)\n",
        "    else:\n",
        "        model = FeedforwardNetwork(\n",
        "            n_classes,\n",
        "            n_feats,\n",
        "            opt['hidden_size'],\n",
        "            opt['layers'],\n",
        "            opt['activation'],\n",
        "            opt['dropout']\n",
        "        )\n",
        "\n",
        "    # get an optimizer\n",
        "    optims = {\"adam\": torch.optim.Adam, \"sgd\": torch.optim.SGD}\n",
        "\n",
        "    optim_cls = optims[opt['optimizer']]\n",
        "    optimizer = optim_cls(\n",
        "        model.parameters(), lr=opt['learning_rate'], weight_decay=opt['l2_decay'], momentum = opt['momentum']\n",
        "    )\n",
        "\n",
        "    # get a loss criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # training loop\n",
        "    epochs = torch.arange(1, opt['epochs'] + 1)\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    print('initial val acc: {:.4f}'.format(evaluate(model, dev_X, dev_y, criterion)[1]))\n",
        "\n",
        "    for ii in epochs:\n",
        "        print('Training epoch {}'.format(ii))\n",
        "        epoch_train_losses = []\n",
        "        for X_batch, y_batch in train_dataloader:\n",
        "            loss = train_batch(\n",
        "                X_batch, y_batch, model, optimizer, criterion)\n",
        "            epoch_train_losses.append(loss)\n",
        "\n",
        "        epoch_train_loss = torch.tensor(epoch_train_losses).mean().item()\n",
        "        val_loss, val_acc = evaluate(model, dev_X, dev_y, criterion)\n",
        "\n",
        "        print('train loss: {:.4f} | val loss: {:.4f} | val acc: {:.4f}'.format(\n",
        "            epoch_train_loss, val_loss, val_acc\n",
        "        ))\n",
        "\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        valid_losses.append(val_loss)\n",
        "        valid_accs.append(val_acc)\n",
        "\n",
        "    elapsed_time = time.time() - start\n",
        "    minutes = int(elapsed_time // 60)\n",
        "    seconds = int(elapsed_time % 60)\n",
        "    print('Training took {} minutes and {} seconds'.format(minutes, seconds))\n",
        "\n",
        "    _, test_acc = evaluate(model, test_X, test_y, criterion)\n",
        "    print('Final test acc: {:.4f}'.format(test_acc))\n",
        "\n",
        "    # plot\n",
        "    if opt['model'] == \"logistic_regression\":\n",
        "        config = (\n",
        "            f\"batch-{opt['batch_size']}-lr-{opt['learning_rate']}-epochs-{opt['epochs']}-\"\n",
        "            f\"l2-{opt['l2_decay']}-opt-{opt['optimizer']}\"\n",
        "        )\n",
        "    else:\n",
        "        config = (\n",
        "            f\"batch-{opt['batch_size']}-lr-{opt['learning_rate']}-epochs-{opt['epochs']}-\"\n",
        "            f\"hidden-{opt['hidden_size']}-dropout-{opt['dropout']}-l2-{opt['l2_decay']}-\"\n",
        "            f\"layers-{opt['layers']}-act-{opt['activation']}-opt-{opt['optimizer']}-mom-{opt['momentum']}\"\n",
        "        )\n",
        "\n",
        "    losses = {\n",
        "        \"Train Loss\": train_losses,\n",
        "        \"Valid Loss\": valid_losses,\n",
        "    }\n",
        "\n",
        "\n",
        "    plot(epochs, losses, filename=f\"{opt['model']}-training-loss-{config}.pdf\")\n",
        "    #accuracy = { \"Valid Accuracy\": valid_accs }\n",
        "    #plot(epochs, accuracy, filename=f\"{opt['model']}-validation-accuracy-{config}.pdf\")\n",
        "    # Plot the results\n",
        "    #plot_results(epochs, train_losses, valid_losses, valid_accs, config, opt)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova sec√ß√£o"
      ],
      "metadata": {
        "id": "7XKSY1CHGmCo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yCz_nIy7SNkx",
        "outputId": "6f92f9a6-9072-40e2-ce9a-0b27203ccf97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial val acc: 0.1645\n",
            "Training epoch 1\n",
            "train loss: 1.7894 | val loss: 1.7798 | val acc: 0.2044\n",
            "Training epoch 2\n",
            "train loss: 1.7791 | val loss: 1.7725 | val acc: 0.2308\n",
            "Training epoch 3\n",
            "train loss: 1.7729 | val loss: 1.7672 | val acc: 0.2286\n",
            "Training epoch 4\n",
            "train loss: 1.7679 | val loss: 1.7624 | val acc: 0.2322\n",
            "Training epoch 5\n",
            "train loss: 1.7640 | val loss: 1.7578 | val acc: 0.2379\n",
            "Training epoch 6\n",
            "train loss: 1.7578 | val loss: 1.7528 | val acc: 0.2415\n",
            "Training epoch 7\n",
            "train loss: 1.7544 | val loss: 1.7480 | val acc: 0.2436\n",
            "Training epoch 8\n",
            "train loss: 1.7505 | val loss: 1.7434 | val acc: 0.2429\n",
            "Training epoch 9\n",
            "train loss: 1.7450 | val loss: 1.7385 | val acc: 0.2443\n",
            "Training epoch 10\n",
            "train loss: 1.7404 | val loss: 1.7337 | val acc: 0.2464\n",
            "Training epoch 11\n",
            "train loss: 1.7366 | val loss: 1.7288 | val acc: 0.2507\n",
            "Training epoch 12\n",
            "train loss: 1.7304 | val loss: 1.7236 | val acc: 0.2543\n",
            "Training epoch 13\n",
            "train loss: 1.7243 | val loss: 1.7184 | val acc: 0.2621\n",
            "Training epoch 14\n",
            "train loss: 1.7208 | val loss: 1.7127 | val acc: 0.2707\n",
            "Training epoch 15\n",
            "train loss: 1.7166 | val loss: 1.7073 | val acc: 0.2778\n",
            "Training epoch 16\n",
            "train loss: 1.7094 | val loss: 1.7019 | val acc: 0.2813\n",
            "Training epoch 17\n",
            "train loss: 1.7045 | val loss: 1.6959 | val acc: 0.2899\n",
            "Training epoch 18\n",
            "train loss: 1.6999 | val loss: 1.6902 | val acc: 0.2970\n",
            "Training epoch 19\n",
            "train loss: 1.6938 | val loss: 1.6842 | val acc: 0.3091\n",
            "Training epoch 20\n",
            "train loss: 1.6878 | val loss: 1.6782 | val acc: 0.3162\n",
            "Training epoch 21\n",
            "train loss: 1.6828 | val loss: 1.6721 | val acc: 0.3219\n",
            "Training epoch 22\n",
            "train loss: 1.6755 | val loss: 1.6663 | val acc: 0.3269\n",
            "Training epoch 23\n",
            "train loss: 1.6723 | val loss: 1.6606 | val acc: 0.3326\n",
            "Training epoch 24\n",
            "train loss: 1.6662 | val loss: 1.6544 | val acc: 0.3376\n",
            "Training epoch 25\n",
            "train loss: 1.6610 | val loss: 1.6484 | val acc: 0.3419\n",
            "Training epoch 26\n",
            "train loss: 1.6544 | val loss: 1.6423 | val acc: 0.3504\n",
            "Training epoch 27\n",
            "train loss: 1.6487 | val loss: 1.6361 | val acc: 0.3604\n",
            "Training epoch 28\n",
            "train loss: 1.6420 | val loss: 1.6302 | val acc: 0.3647\n",
            "Training epoch 29\n",
            "train loss: 1.6351 | val loss: 1.6238 | val acc: 0.3725\n",
            "Training epoch 30\n",
            "train loss: 1.6308 | val loss: 1.6176 | val acc: 0.3754\n",
            "Training epoch 31\n",
            "train loss: 1.6248 | val loss: 1.6113 | val acc: 0.3853\n",
            "Training epoch 32\n",
            "train loss: 1.6184 | val loss: 1.6054 | val acc: 0.3853\n",
            "Training epoch 33\n",
            "train loss: 1.6111 | val loss: 1.5987 | val acc: 0.3960\n",
            "Training epoch 34\n",
            "train loss: 1.6050 | val loss: 1.5920 | val acc: 0.3953\n",
            "Training epoch 35\n",
            "train loss: 1.5999 | val loss: 1.5859 | val acc: 0.3967\n",
            "Training epoch 36\n",
            "train loss: 1.5949 | val loss: 1.5799 | val acc: 0.4010\n",
            "Training epoch 37\n",
            "train loss: 1.5884 | val loss: 1.5736 | val acc: 0.4017\n",
            "Training epoch 38\n",
            "train loss: 1.5852 | val loss: 1.5673 | val acc: 0.4110\n",
            "Training epoch 39\n",
            "train loss: 1.5764 | val loss: 1.5611 | val acc: 0.4110\n",
            "Training epoch 40\n",
            "train loss: 1.5694 | val loss: 1.5554 | val acc: 0.4095\n",
            "Training epoch 41\n",
            "train loss: 1.5635 | val loss: 1.5487 | val acc: 0.4167\n",
            "Training epoch 42\n",
            "train loss: 1.5580 | val loss: 1.5427 | val acc: 0.4209\n",
            "Training epoch 43\n",
            "train loss: 1.5529 | val loss: 1.5374 | val acc: 0.4217\n",
            "Training epoch 44\n",
            "train loss: 1.5472 | val loss: 1.5312 | val acc: 0.4252\n",
            "Training epoch 45\n",
            "train loss: 1.5386 | val loss: 1.5253 | val acc: 0.4288\n",
            "Training epoch 46\n",
            "train loss: 1.5355 | val loss: 1.5196 | val acc: 0.4309\n",
            "Training epoch 47\n",
            "train loss: 1.5313 | val loss: 1.5142 | val acc: 0.4345\n",
            "Training epoch 48\n",
            "train loss: 1.5237 | val loss: 1.5092 | val acc: 0.4352\n",
            "Training epoch 49\n",
            "train loss: 1.5204 | val loss: 1.5032 | val acc: 0.4323\n",
            "Training epoch 50\n",
            "train loss: 1.5154 | val loss: 1.4983 | val acc: 0.4359\n",
            "Training epoch 51\n",
            "train loss: 1.5098 | val loss: 1.4928 | val acc: 0.4395\n",
            "Training epoch 52\n",
            "train loss: 1.5024 | val loss: 1.4880 | val acc: 0.4416\n",
            "Training epoch 53\n",
            "train loss: 1.5000 | val loss: 1.4827 | val acc: 0.4380\n",
            "Training epoch 54\n",
            "train loss: 1.4977 | val loss: 1.4783 | val acc: 0.4437\n",
            "Training epoch 55\n",
            "train loss: 1.4900 | val loss: 1.4742 | val acc: 0.4444\n",
            "Training epoch 56\n",
            "train loss: 1.4835 | val loss: 1.4693 | val acc: 0.4416\n",
            "Training epoch 57\n",
            "train loss: 1.4811 | val loss: 1.4652 | val acc: 0.4423\n",
            "Training epoch 58\n",
            "train loss: 1.4810 | val loss: 1.4618 | val acc: 0.4452\n",
            "Training epoch 59\n",
            "train loss: 1.4706 | val loss: 1.4568 | val acc: 0.4480\n",
            "Training epoch 60\n",
            "train loss: 1.4719 | val loss: 1.4530 | val acc: 0.4466\n",
            "Training epoch 61\n",
            "train loss: 1.4624 | val loss: 1.4480 | val acc: 0.4452\n",
            "Training epoch 62\n",
            "train loss: 1.4597 | val loss: 1.4446 | val acc: 0.4480\n",
            "Training epoch 63\n",
            "train loss: 1.4591 | val loss: 1.4410 | val acc: 0.4494\n",
            "Training epoch 64\n",
            "train loss: 1.4552 | val loss: 1.4377 | val acc: 0.4509\n",
            "Training epoch 65\n",
            "train loss: 1.4500 | val loss: 1.4338 | val acc: 0.4501\n",
            "Training epoch 66\n",
            "train loss: 1.4481 | val loss: 1.4305 | val acc: 0.4509\n",
            "Training epoch 67\n",
            "train loss: 1.4436 | val loss: 1.4275 | val acc: 0.4473\n",
            "Training epoch 68\n",
            "train loss: 1.4394 | val loss: 1.4238 | val acc: 0.4544\n",
            "Training epoch 69\n",
            "train loss: 1.4349 | val loss: 1.4206 | val acc: 0.4558\n",
            "Training epoch 70\n",
            "train loss: 1.4351 | val loss: 1.4184 | val acc: 0.4516\n",
            "Training epoch 71\n",
            "train loss: 1.4315 | val loss: 1.4151 | val acc: 0.4566\n",
            "Training epoch 72\n",
            "train loss: 1.4262 | val loss: 1.4124 | val acc: 0.4558\n",
            "Training epoch 73\n",
            "train loss: 1.4203 | val loss: 1.4091 | val acc: 0.4587\n",
            "Training epoch 74\n",
            "train loss: 1.4189 | val loss: 1.4068 | val acc: 0.4580\n",
            "Training epoch 75\n",
            "train loss: 1.4141 | val loss: 1.4038 | val acc: 0.4587\n",
            "Training epoch 76\n",
            "train loss: 1.4142 | val loss: 1.4009 | val acc: 0.4580\n",
            "Training epoch 77\n",
            "train loss: 1.4137 | val loss: 1.3989 | val acc: 0.4601\n",
            "Training epoch 78\n",
            "train loss: 1.4091 | val loss: 1.3962 | val acc: 0.4580\n",
            "Training epoch 79\n",
            "train loss: 1.4115 | val loss: 1.3944 | val acc: 0.4608\n",
            "Training epoch 80\n",
            "train loss: 1.4056 | val loss: 1.3923 | val acc: 0.4601\n",
            "Training epoch 81\n",
            "train loss: 1.4035 | val loss: 1.3903 | val acc: 0.4608\n",
            "Training epoch 82\n",
            "train loss: 1.3984 | val loss: 1.3873 | val acc: 0.4615\n",
            "Training epoch 83\n",
            "train loss: 1.3982 | val loss: 1.3854 | val acc: 0.4637\n",
            "Training epoch 84\n",
            "train loss: 1.3957 | val loss: 1.3834 | val acc: 0.4615\n",
            "Training epoch 85\n",
            "train loss: 1.3910 | val loss: 1.3811 | val acc: 0.4615\n",
            "Training epoch 86\n",
            "train loss: 1.3849 | val loss: 1.3792 | val acc: 0.4608\n",
            "Training epoch 87\n",
            "train loss: 1.3919 | val loss: 1.3776 | val acc: 0.4630\n",
            "Training epoch 88\n",
            "train loss: 1.3909 | val loss: 1.3756 | val acc: 0.4623\n",
            "Training epoch 89\n",
            "train loss: 1.3870 | val loss: 1.3745 | val acc: 0.4665\n",
            "Training epoch 90\n",
            "train loss: 1.3803 | val loss: 1.3721 | val acc: 0.4630\n",
            "Training epoch 91\n",
            "train loss: 1.3819 | val loss: 1.3704 | val acc: 0.4658\n",
            "Training epoch 92\n",
            "train loss: 1.3765 | val loss: 1.3680 | val acc: 0.4672\n",
            "Training epoch 93\n",
            "train loss: 1.3755 | val loss: 1.3668 | val acc: 0.4658\n",
            "Training epoch 94\n",
            "train loss: 1.3731 | val loss: 1.3646 | val acc: 0.4658\n",
            "Training epoch 95\n",
            "train loss: 1.3743 | val loss: 1.3633 | val acc: 0.4665\n",
            "Training epoch 96\n",
            "train loss: 1.3721 | val loss: 1.3614 | val acc: 0.4687\n",
            "Training epoch 97\n",
            "train loss: 1.3667 | val loss: 1.3600 | val acc: 0.4679\n",
            "Training epoch 98\n",
            "train loss: 1.3665 | val loss: 1.3580 | val acc: 0.4672\n",
            "Training epoch 99\n",
            "train loss: 1.3695 | val loss: 1.3567 | val acc: 0.4687\n",
            "Training epoch 100\n",
            "train loss: 1.3659 | val loss: 1.3561 | val acc: 0.4672\n",
            "Training epoch 101\n",
            "train loss: 1.3619 | val loss: 1.3541 | val acc: 0.4665\n",
            "Training epoch 102\n",
            "train loss: 1.3606 | val loss: 1.3534 | val acc: 0.4665\n",
            "Training epoch 103\n",
            "train loss: 1.3583 | val loss: 1.3511 | val acc: 0.4687\n",
            "Training epoch 104\n",
            "train loss: 1.3586 | val loss: 1.3498 | val acc: 0.4679\n",
            "Training epoch 105\n",
            "train loss: 1.3552 | val loss: 1.3485 | val acc: 0.4679\n",
            "Training epoch 106\n",
            "train loss: 1.3539 | val loss: 1.3464 | val acc: 0.4694\n",
            "Training epoch 107\n",
            "train loss: 1.3521 | val loss: 1.3459 | val acc: 0.4679\n",
            "Training epoch 108\n",
            "train loss: 1.3514 | val loss: 1.3441 | val acc: 0.4701\n",
            "Training epoch 109\n",
            "train loss: 1.3528 | val loss: 1.3437 | val acc: 0.4679\n",
            "Training epoch 110\n",
            "train loss: 1.3445 | val loss: 1.3422 | val acc: 0.4679\n",
            "Training epoch 111\n",
            "train loss: 1.3451 | val loss: 1.3400 | val acc: 0.4665\n",
            "Training epoch 112\n",
            "train loss: 1.3434 | val loss: 1.3389 | val acc: 0.4687\n",
            "Training epoch 113\n",
            "train loss: 1.3482 | val loss: 1.3378 | val acc: 0.4701\n",
            "Training epoch 114\n",
            "train loss: 1.3451 | val loss: 1.3370 | val acc: 0.4694\n",
            "Training epoch 115\n",
            "train loss: 1.3429 | val loss: 1.3355 | val acc: 0.4679\n",
            "Training epoch 116\n",
            "train loss: 1.3397 | val loss: 1.3348 | val acc: 0.4694\n",
            "Training epoch 117\n",
            "train loss: 1.3386 | val loss: 1.3338 | val acc: 0.4679\n",
            "Training epoch 118\n",
            "train loss: 1.3363 | val loss: 1.3315 | val acc: 0.4708\n",
            "Training epoch 119\n",
            "train loss: 1.3359 | val loss: 1.3314 | val acc: 0.4665\n",
            "Training epoch 120\n",
            "train loss: 1.3346 | val loss: 1.3290 | val acc: 0.4722\n",
            "Training epoch 121\n",
            "train loss: 1.3304 | val loss: 1.3282 | val acc: 0.4708\n",
            "Training epoch 122\n",
            "train loss: 1.3295 | val loss: 1.3271 | val acc: 0.4722\n",
            "Training epoch 123\n",
            "train loss: 1.3314 | val loss: 1.3264 | val acc: 0.4744\n",
            "Training epoch 124\n",
            "train loss: 1.3290 | val loss: 1.3250 | val acc: 0.4736\n",
            "Training epoch 125\n",
            "train loss: 1.3237 | val loss: 1.3236 | val acc: 0.4751\n",
            "Training epoch 126\n",
            "train loss: 1.3294 | val loss: 1.3234 | val acc: 0.4722\n",
            "Training epoch 127\n",
            "train loss: 1.3266 | val loss: 1.3215 | val acc: 0.4765\n",
            "Training epoch 128\n",
            "train loss: 1.3237 | val loss: 1.3207 | val acc: 0.4758\n",
            "Training epoch 129\n",
            "train loss: 1.3274 | val loss: 1.3195 | val acc: 0.4793\n",
            "Training epoch 130\n",
            "train loss: 1.3199 | val loss: 1.3182 | val acc: 0.4772\n",
            "Training epoch 131\n",
            "train loss: 1.3214 | val loss: 1.3172 | val acc: 0.4801\n",
            "Training epoch 132\n",
            "train loss: 1.3198 | val loss: 1.3169 | val acc: 0.4744\n",
            "Training epoch 133\n",
            "train loss: 1.3217 | val loss: 1.3154 | val acc: 0.4801\n",
            "Training epoch 134\n",
            "train loss: 1.3218 | val loss: 1.3143 | val acc: 0.4808\n",
            "Training epoch 135\n",
            "train loss: 1.3148 | val loss: 1.3131 | val acc: 0.4815\n",
            "Training epoch 136\n",
            "train loss: 1.3158 | val loss: 1.3127 | val acc: 0.4801\n",
            "Training epoch 137\n",
            "train loss: 1.3146 | val loss: 1.3115 | val acc: 0.4808\n",
            "Training epoch 138\n",
            "train loss: 1.3115 | val loss: 1.3105 | val acc: 0.4808\n",
            "Training epoch 139\n",
            "train loss: 1.3130 | val loss: 1.3096 | val acc: 0.4801\n",
            "Training epoch 140\n",
            "train loss: 1.3105 | val loss: 1.3079 | val acc: 0.4843\n",
            "Training epoch 141\n",
            "train loss: 1.3090 | val loss: 1.3073 | val acc: 0.4815\n",
            "Training epoch 142\n",
            "train loss: 1.3085 | val loss: 1.3070 | val acc: 0.4801\n",
            "Training epoch 143\n",
            "train loss: 1.3086 | val loss: 1.3056 | val acc: 0.4822\n",
            "Training epoch 144\n",
            "train loss: 1.3063 | val loss: 1.3047 | val acc: 0.4850\n",
            "Training epoch 145\n",
            "train loss: 1.3041 | val loss: 1.3035 | val acc: 0.4829\n",
            "Training epoch 146\n",
            "train loss: 1.3013 | val loss: 1.3029 | val acc: 0.4822\n",
            "Training epoch 147\n",
            "train loss: 1.3045 | val loss: 1.3018 | val acc: 0.4850\n",
            "Training epoch 148\n",
            "train loss: 1.3035 | val loss: 1.3014 | val acc: 0.4822\n",
            "Training epoch 149\n",
            "train loss: 1.2941 | val loss: 1.2991 | val acc: 0.4865\n",
            "Training epoch 150\n",
            "train loss: 1.2991 | val loss: 1.2986 | val acc: 0.4886\n",
            "Training epoch 151\n",
            "train loss: 1.2996 | val loss: 1.2980 | val acc: 0.4850\n",
            "Training epoch 152\n",
            "train loss: 1.2940 | val loss: 1.2968 | val acc: 0.4865\n",
            "Training epoch 153\n",
            "train loss: 1.2942 | val loss: 1.2958 | val acc: 0.4879\n",
            "Training epoch 154\n",
            "train loss: 1.2926 | val loss: 1.2950 | val acc: 0.4858\n",
            "Training epoch 155\n",
            "train loss: 1.2950 | val loss: 1.2949 | val acc: 0.4843\n",
            "Training epoch 156\n",
            "train loss: 1.2979 | val loss: 1.2936 | val acc: 0.4872\n",
            "Training epoch 157\n",
            "train loss: 1.2898 | val loss: 1.2915 | val acc: 0.4872\n",
            "Training epoch 158\n",
            "train loss: 1.2905 | val loss: 1.2919 | val acc: 0.4886\n",
            "Training epoch 159\n",
            "train loss: 1.2895 | val loss: 1.2901 | val acc: 0.4893\n",
            "Training epoch 160\n",
            "train loss: 1.2887 | val loss: 1.2895 | val acc: 0.4879\n",
            "Training epoch 161\n",
            "train loss: 1.2882 | val loss: 1.2875 | val acc: 0.4893\n",
            "Training epoch 162\n",
            "train loss: 1.2882 | val loss: 1.2868 | val acc: 0.4879\n",
            "Training epoch 163\n",
            "train loss: 1.2824 | val loss: 1.2865 | val acc: 0.4907\n",
            "Training epoch 164\n",
            "train loss: 1.2860 | val loss: 1.2856 | val acc: 0.4907\n",
            "Training epoch 165\n",
            "train loss: 1.2821 | val loss: 1.2857 | val acc: 0.4886\n",
            "Training epoch 166\n",
            "train loss: 1.2829 | val loss: 1.2835 | val acc: 0.4915\n",
            "Training epoch 167\n",
            "train loss: 1.2846 | val loss: 1.2831 | val acc: 0.4943\n",
            "Training epoch 168\n",
            "train loss: 1.2812 | val loss: 1.2823 | val acc: 0.4943\n",
            "Training epoch 169\n",
            "train loss: 1.2822 | val loss: 1.2828 | val acc: 0.4886\n",
            "Training epoch 170\n",
            "train loss: 1.2800 | val loss: 1.2803 | val acc: 0.4950\n",
            "Training epoch 171\n",
            "train loss: 1.2823 | val loss: 1.2805 | val acc: 0.4900\n",
            "Training epoch 172\n",
            "train loss: 1.2787 | val loss: 1.2795 | val acc: 0.4979\n",
            "Training epoch 173\n",
            "train loss: 1.2743 | val loss: 1.2781 | val acc: 0.4979\n",
            "Training epoch 174\n",
            "train loss: 1.2759 | val loss: 1.2779 | val acc: 0.4929\n",
            "Training epoch 175\n",
            "train loss: 1.2762 | val loss: 1.2768 | val acc: 0.4936\n",
            "Training epoch 176\n",
            "train loss: 1.2736 | val loss: 1.2761 | val acc: 0.4922\n",
            "Training epoch 177\n",
            "train loss: 1.2717 | val loss: 1.2736 | val acc: 0.4964\n",
            "Training epoch 178\n",
            "train loss: 1.2731 | val loss: 1.2732 | val acc: 0.4986\n",
            "Training epoch 179\n",
            "train loss: 1.2737 | val loss: 1.2731 | val acc: 0.4957\n",
            "Training epoch 180\n",
            "train loss: 1.2697 | val loss: 1.2721 | val acc: 0.4957\n",
            "Training epoch 181\n",
            "train loss: 1.2691 | val loss: 1.2708 | val acc: 0.4964\n",
            "Training epoch 182\n",
            "train loss: 1.2712 | val loss: 1.2704 | val acc: 0.4979\n",
            "Training epoch 183\n",
            "train loss: 1.2693 | val loss: 1.2699 | val acc: 0.4943\n",
            "Training epoch 184\n",
            "train loss: 1.2693 | val loss: 1.2690 | val acc: 0.4979\n",
            "Training epoch 185\n",
            "train loss: 1.2658 | val loss: 1.2672 | val acc: 0.5014\n",
            "Training epoch 186\n",
            "train loss: 1.2709 | val loss: 1.2670 | val acc: 0.5007\n",
            "Training epoch 187\n",
            "train loss: 1.2678 | val loss: 1.2667 | val acc: 0.4964\n",
            "Training epoch 188\n",
            "train loss: 1.2625 | val loss: 1.2656 | val acc: 0.4964\n",
            "Training epoch 189\n",
            "train loss: 1.2618 | val loss: 1.2654 | val acc: 0.4972\n",
            "Training epoch 190\n",
            "train loss: 1.2617 | val loss: 1.2646 | val acc: 0.5000\n",
            "Training epoch 191\n",
            "train loss: 1.2625 | val loss: 1.2644 | val acc: 0.4964\n",
            "Training epoch 192\n",
            "train loss: 1.2608 | val loss: 1.2621 | val acc: 0.5043\n",
            "Training epoch 193\n",
            "train loss: 1.2611 | val loss: 1.2613 | val acc: 0.5014\n",
            "Training epoch 194\n",
            "train loss: 1.2624 | val loss: 1.2612 | val acc: 0.5007\n",
            "Training epoch 195\n",
            "train loss: 1.2598 | val loss: 1.2607 | val acc: 0.5007\n",
            "Training epoch 196\n",
            "train loss: 1.2602 | val loss: 1.2588 | val acc: 0.5050\n",
            "Training epoch 197\n",
            "train loss: 1.2532 | val loss: 1.2583 | val acc: 0.5021\n",
            "Training epoch 198\n",
            "train loss: 1.2566 | val loss: 1.2583 | val acc: 0.5014\n",
            "Training epoch 199\n",
            "train loss: 1.2546 | val loss: 1.2564 | val acc: 0.5043\n",
            "Training epoch 200\n",
            "train loss: 1.2551 | val loss: 1.2562 | val acc: 0.5028\n",
            "Training epoch 201\n",
            "train loss: 1.2565 | val loss: 1.2558 | val acc: 0.5007\n",
            "Training epoch 202\n",
            "train loss: 1.2539 | val loss: 1.2560 | val acc: 0.5021\n",
            "Training epoch 203\n",
            "train loss: 1.2526 | val loss: 1.2539 | val acc: 0.5050\n",
            "Training epoch 204\n",
            "train loss: 1.2507 | val loss: 1.2542 | val acc: 0.5050\n",
            "Training epoch 205\n",
            "train loss: 1.2507 | val loss: 1.2524 | val acc: 0.5057\n",
            "Training epoch 206\n",
            "train loss: 1.2527 | val loss: 1.2518 | val acc: 0.5036\n",
            "Training epoch 207\n",
            "train loss: 1.2537 | val loss: 1.2510 | val acc: 0.5050\n",
            "Training epoch 208\n",
            "train loss: 1.2505 | val loss: 1.2518 | val acc: 0.5043\n",
            "Training epoch 209\n",
            "train loss: 1.2462 | val loss: 1.2498 | val acc: 0.5028\n",
            "Training epoch 210\n",
            "train loss: 1.2429 | val loss: 1.2481 | val acc: 0.5085\n",
            "Training epoch 211\n",
            "train loss: 1.2453 | val loss: 1.2481 | val acc: 0.5107\n",
            "Training epoch 212\n",
            "train loss: 1.2430 | val loss: 1.2484 | val acc: 0.5064\n",
            "Training epoch 213\n",
            "train loss: 1.2462 | val loss: 1.2466 | val acc: 0.5085\n",
            "Training epoch 214\n",
            "train loss: 1.2405 | val loss: 1.2465 | val acc: 0.5064\n",
            "Training epoch 215\n",
            "train loss: 1.2410 | val loss: 1.2453 | val acc: 0.5050\n",
            "Training epoch 216\n",
            "train loss: 1.2399 | val loss: 1.2450 | val acc: 0.5064\n",
            "Training epoch 217\n",
            "train loss: 1.2416 | val loss: 1.2447 | val acc: 0.5135\n",
            "Training epoch 218\n",
            "train loss: 1.2402 | val loss: 1.2430 | val acc: 0.5128\n",
            "Training epoch 219\n",
            "train loss: 1.2372 | val loss: 1.2425 | val acc: 0.5121\n",
            "Training epoch 220\n",
            "train loss: 1.2383 | val loss: 1.2414 | val acc: 0.5085\n",
            "Training epoch 221\n",
            "train loss: 1.2385 | val loss: 1.2424 | val acc: 0.5114\n",
            "Training epoch 222\n",
            "train loss: 1.2379 | val loss: 1.2407 | val acc: 0.5121\n",
            "Training epoch 223\n",
            "train loss: 1.2382 | val loss: 1.2395 | val acc: 0.5093\n",
            "Training epoch 224\n",
            "train loss: 1.2360 | val loss: 1.2391 | val acc: 0.5107\n",
            "Training epoch 225\n",
            "train loss: 1.2360 | val loss: 1.2391 | val acc: 0.5064\n",
            "Training epoch 226\n",
            "train loss: 1.2373 | val loss: 1.2379 | val acc: 0.5093\n",
            "Training epoch 227\n",
            "train loss: 1.2345 | val loss: 1.2362 | val acc: 0.5121\n",
            "Training epoch 228\n",
            "train loss: 1.2360 | val loss: 1.2375 | val acc: 0.5100\n",
            "Training epoch 229\n",
            "train loss: 1.2335 | val loss: 1.2368 | val acc: 0.5093\n",
            "Training epoch 230\n",
            "train loss: 1.2344 | val loss: 1.2361 | val acc: 0.5135\n",
            "Training epoch 231\n",
            "train loss: 1.2313 | val loss: 1.2352 | val acc: 0.5071\n",
            "Training epoch 232\n",
            "train loss: 1.2327 | val loss: 1.2340 | val acc: 0.5128\n",
            "Training epoch 233\n",
            "train loss: 1.2275 | val loss: 1.2341 | val acc: 0.5121\n",
            "Training epoch 234\n",
            "train loss: 1.2299 | val loss: 1.2331 | val acc: 0.5150\n",
            "Training epoch 235\n",
            "train loss: 1.2273 | val loss: 1.2318 | val acc: 0.5128\n",
            "Training epoch 236\n",
            "train loss: 1.2254 | val loss: 1.2313 | val acc: 0.5150\n",
            "Training epoch 237\n",
            "train loss: 1.2268 | val loss: 1.2315 | val acc: 0.5114\n",
            "Training epoch 238\n",
            "train loss: 1.2247 | val loss: 1.2294 | val acc: 0.5171\n",
            "Training epoch 239\n",
            "train loss: 1.2260 | val loss: 1.2293 | val acc: 0.5150\n",
            "Training epoch 240\n",
            "train loss: 1.2243 | val loss: 1.2285 | val acc: 0.5150\n",
            "Training epoch 241\n",
            "train loss: 1.2260 | val loss: 1.2287 | val acc: 0.5121\n",
            "Training epoch 242\n",
            "train loss: 1.2212 | val loss: 1.2274 | val acc: 0.5135\n",
            "Training epoch 243\n",
            "train loss: 1.2221 | val loss: 1.2269 | val acc: 0.5135\n",
            "Training epoch 244\n",
            "train loss: 1.2216 | val loss: 1.2268 | val acc: 0.5128\n",
            "Training epoch 245\n",
            "train loss: 1.2227 | val loss: 1.2259 | val acc: 0.5142\n",
            "Training epoch 246\n",
            "train loss: 1.2209 | val loss: 1.2252 | val acc: 0.5150\n",
            "Training epoch 247\n",
            "train loss: 1.2193 | val loss: 1.2253 | val acc: 0.5199\n",
            "Training epoch 248\n",
            "train loss: 1.2190 | val loss: 1.2241 | val acc: 0.5171\n",
            "Training epoch 249\n",
            "train loss: 1.2189 | val loss: 1.2242 | val acc: 0.5142\n",
            "Training epoch 250\n",
            "train loss: 1.2197 | val loss: 1.2235 | val acc: 0.5142\n",
            "Training epoch 251\n",
            "train loss: 1.2139 | val loss: 1.2227 | val acc: 0.5164\n",
            "Training epoch 252\n",
            "train loss: 1.2157 | val loss: 1.2219 | val acc: 0.5171\n",
            "Training epoch 253\n",
            "train loss: 1.2132 | val loss: 1.2212 | val acc: 0.5142\n",
            "Training epoch 254\n",
            "train loss: 1.2169 | val loss: 1.2207 | val acc: 0.5150\n",
            "Training epoch 255\n",
            "train loss: 1.2123 | val loss: 1.2198 | val acc: 0.5185\n",
            "Training epoch 256\n",
            "train loss: 1.2129 | val loss: 1.2200 | val acc: 0.5171\n",
            "Training epoch 257\n",
            "train loss: 1.2133 | val loss: 1.2194 | val acc: 0.5214\n",
            "Training epoch 258\n",
            "train loss: 1.2161 | val loss: 1.2191 | val acc: 0.5150\n",
            "Training epoch 259\n",
            "train loss: 1.2153 | val loss: 1.2175 | val acc: 0.5178\n",
            "Training epoch 260\n",
            "train loss: 1.2103 | val loss: 1.2174 | val acc: 0.5207\n",
            "Training epoch 261\n",
            "train loss: 1.2103 | val loss: 1.2170 | val acc: 0.5185\n",
            "Training epoch 262\n",
            "train loss: 1.2132 | val loss: 1.2167 | val acc: 0.5207\n",
            "Training epoch 263\n",
            "train loss: 1.2105 | val loss: 1.2158 | val acc: 0.5185\n",
            "Training epoch 264\n",
            "train loss: 1.2059 | val loss: 1.2146 | val acc: 0.5221\n",
            "Training epoch 265\n",
            "train loss: 1.2097 | val loss: 1.2141 | val acc: 0.5235\n",
            "Training epoch 266\n",
            "train loss: 1.2059 | val loss: 1.2139 | val acc: 0.5207\n",
            "Training epoch 267\n",
            "train loss: 1.2064 | val loss: 1.2140 | val acc: 0.5185\n",
            "Training epoch 268\n",
            "train loss: 1.2052 | val loss: 1.2129 | val acc: 0.5207\n",
            "Training epoch 269\n",
            "train loss: 1.2031 | val loss: 1.2124 | val acc: 0.5242\n",
            "Training epoch 270\n",
            "train loss: 1.1993 | val loss: 1.2112 | val acc: 0.5235\n",
            "Training epoch 271\n",
            "train loss: 1.2055 | val loss: 1.2102 | val acc: 0.5242\n",
            "Training epoch 272\n",
            "train loss: 1.2038 | val loss: 1.2103 | val acc: 0.5228\n",
            "Training epoch 273\n",
            "train loss: 1.2053 | val loss: 1.2096 | val acc: 0.5207\n",
            "Training epoch 274\n",
            "train loss: 1.1980 | val loss: 1.2092 | val acc: 0.5242\n",
            "Training epoch 275\n",
            "train loss: 1.1998 | val loss: 1.2086 | val acc: 0.5292\n",
            "Training epoch 276\n",
            "train loss: 1.2009 | val loss: 1.2082 | val acc: 0.5264\n",
            "Training epoch 277\n",
            "train loss: 1.1989 | val loss: 1.2083 | val acc: 0.5256\n",
            "Training epoch 278\n",
            "train loss: 1.1975 | val loss: 1.2078 | val acc: 0.5256\n",
            "Training epoch 279\n",
            "train loss: 1.2008 | val loss: 1.2067 | val acc: 0.5249\n",
            "Training epoch 280\n",
            "train loss: 1.1983 | val loss: 1.2064 | val acc: 0.5264\n",
            "Training epoch 281\n",
            "train loss: 1.1984 | val loss: 1.2067 | val acc: 0.5249\n",
            "Training epoch 282\n",
            "train loss: 1.1952 | val loss: 1.2047 | val acc: 0.5313\n",
            "Training epoch 283\n",
            "train loss: 1.1957 | val loss: 1.2050 | val acc: 0.5235\n",
            "Training epoch 284\n",
            "train loss: 1.1960 | val loss: 1.2040 | val acc: 0.5278\n",
            "Training epoch 285\n",
            "train loss: 1.1937 | val loss: 1.2033 | val acc: 0.5306\n",
            "Training epoch 286\n",
            "train loss: 1.1961 | val loss: 1.2032 | val acc: 0.5299\n",
            "Training epoch 287\n",
            "train loss: 1.1973 | val loss: 1.2034 | val acc: 0.5349\n",
            "Training epoch 288\n",
            "train loss: 1.1932 | val loss: 1.2032 | val acc: 0.5285\n",
            "Training epoch 289\n",
            "train loss: 1.1914 | val loss: 1.2016 | val acc: 0.5278\n",
            "Training epoch 290\n",
            "train loss: 1.1926 | val loss: 1.2009 | val acc: 0.5313\n",
            "Training epoch 291\n",
            "train loss: 1.1909 | val loss: 1.2005 | val acc: 0.5313\n",
            "Training epoch 292\n",
            "train loss: 1.1934 | val loss: 1.2003 | val acc: 0.5349\n",
            "Training epoch 293\n",
            "train loss: 1.1900 | val loss: 1.2001 | val acc: 0.5228\n",
            "Training epoch 294\n",
            "train loss: 1.1907 | val loss: 1.1991 | val acc: 0.5271\n",
            "Training epoch 295\n",
            "train loss: 1.1912 | val loss: 1.1993 | val acc: 0.5306\n",
            "Training epoch 296\n",
            "train loss: 1.1907 | val loss: 1.1990 | val acc: 0.5321\n",
            "Training epoch 297\n",
            "train loss: 1.1886 | val loss: 1.1985 | val acc: 0.5313\n",
            "Training epoch 298\n",
            "train loss: 1.1866 | val loss: 1.1976 | val acc: 0.5328\n",
            "Training epoch 299\n",
            "train loss: 1.1881 | val loss: 1.1974 | val acc: 0.5392\n",
            "Training epoch 300\n",
            "train loss: 1.1854 | val loss: 1.1970 | val acc: 0.5356\n",
            "Training epoch 301\n",
            "train loss: 1.1838 | val loss: 1.1966 | val acc: 0.5349\n",
            "Training epoch 302\n",
            "train loss: 1.1837 | val loss: 1.1951 | val acc: 0.5399\n",
            "Training epoch 303\n",
            "train loss: 1.1833 | val loss: 1.1952 | val acc: 0.5406\n",
            "Training epoch 304\n",
            "train loss: 1.1846 | val loss: 1.1948 | val acc: 0.5363\n",
            "Training epoch 305\n",
            "train loss: 1.1816 | val loss: 1.1947 | val acc: 0.5321\n",
            "Training epoch 306\n",
            "train loss: 1.1844 | val loss: 1.1946 | val acc: 0.5342\n",
            "Training epoch 307\n",
            "train loss: 1.1830 | val loss: 1.1934 | val acc: 0.5392\n",
            "Training epoch 308\n",
            "train loss: 1.1844 | val loss: 1.1929 | val acc: 0.5363\n",
            "Training epoch 309\n",
            "train loss: 1.1831 | val loss: 1.1927 | val acc: 0.5313\n",
            "Training epoch 310\n",
            "train loss: 1.1786 | val loss: 1.1929 | val acc: 0.5385\n",
            "Training epoch 311\n",
            "train loss: 1.1831 | val loss: 1.1932 | val acc: 0.5306\n",
            "Training epoch 312\n",
            "train loss: 1.1793 | val loss: 1.1925 | val acc: 0.5377\n",
            "Training epoch 313\n",
            "train loss: 1.1771 | val loss: 1.1909 | val acc: 0.5342\n",
            "Training epoch 314\n",
            "train loss: 1.1805 | val loss: 1.1900 | val acc: 0.5392\n",
            "Training epoch 315\n",
            "train loss: 1.1798 | val loss: 1.1910 | val acc: 0.5385\n",
            "Training epoch 316\n",
            "train loss: 1.1762 | val loss: 1.1888 | val acc: 0.5413\n",
            "Training epoch 317\n",
            "train loss: 1.1810 | val loss: 1.1887 | val acc: 0.5321\n",
            "Training epoch 318\n",
            "train loss: 1.1736 | val loss: 1.1887 | val acc: 0.5406\n",
            "Training epoch 319\n",
            "train loss: 1.1731 | val loss: 1.1876 | val acc: 0.5385\n",
            "Training epoch 320\n",
            "train loss: 1.1759 | val loss: 1.1885 | val acc: 0.5413\n",
            "Training epoch 321\n",
            "train loss: 1.1729 | val loss: 1.1867 | val acc: 0.5434\n",
            "Training epoch 322\n",
            "train loss: 1.1741 | val loss: 1.1859 | val acc: 0.5385\n",
            "Training epoch 323\n",
            "train loss: 1.1745 | val loss: 1.1867 | val acc: 0.5299\n",
            "Training epoch 324\n",
            "train loss: 1.1713 | val loss: 1.1857 | val acc: 0.5442\n",
            "Training epoch 325\n",
            "train loss: 1.1714 | val loss: 1.1851 | val acc: 0.5392\n",
            "Training epoch 326\n",
            "train loss: 1.1726 | val loss: 1.1842 | val acc: 0.5399\n",
            "Training epoch 327\n",
            "train loss: 1.1766 | val loss: 1.1847 | val acc: 0.5406\n",
            "Training epoch 328\n",
            "train loss: 1.1727 | val loss: 1.1839 | val acc: 0.5335\n",
            "Training epoch 329\n",
            "train loss: 1.1691 | val loss: 1.1830 | val acc: 0.5442\n",
            "Training epoch 330\n",
            "train loss: 1.1695 | val loss: 1.1835 | val acc: 0.5377\n",
            "Training epoch 331\n",
            "train loss: 1.1698 | val loss: 1.1831 | val acc: 0.5356\n",
            "Training epoch 332\n",
            "train loss: 1.1715 | val loss: 1.1833 | val acc: 0.5377\n",
            "Training epoch 333\n",
            "train loss: 1.1706 | val loss: 1.1817 | val acc: 0.5392\n",
            "Training epoch 334\n",
            "train loss: 1.1677 | val loss: 1.1805 | val acc: 0.5399\n",
            "Training epoch 335\n",
            "train loss: 1.1678 | val loss: 1.1821 | val acc: 0.5377\n",
            "Training epoch 336\n",
            "train loss: 1.1637 | val loss: 1.1805 | val acc: 0.5442\n",
            "Training epoch 337\n",
            "train loss: 1.1675 | val loss: 1.1804 | val acc: 0.5427\n",
            "Training epoch 338\n",
            "train loss: 1.1693 | val loss: 1.1803 | val acc: 0.5484\n",
            "Training epoch 339\n",
            "train loss: 1.1630 | val loss: 1.1794 | val acc: 0.5321\n",
            "Training epoch 340\n",
            "train loss: 1.1645 | val loss: 1.1781 | val acc: 0.5406\n",
            "Training epoch 341\n",
            "train loss: 1.1640 | val loss: 1.1780 | val acc: 0.5470\n",
            "Training epoch 342\n",
            "train loss: 1.1624 | val loss: 1.1783 | val acc: 0.5470\n",
            "Training epoch 343\n",
            "train loss: 1.1660 | val loss: 1.1772 | val acc: 0.5463\n",
            "Training epoch 344\n",
            "train loss: 1.1598 | val loss: 1.1771 | val acc: 0.5363\n",
            "Training epoch 345\n",
            "train loss: 1.1623 | val loss: 1.1779 | val acc: 0.5442\n",
            "Training epoch 346\n",
            "train loss: 1.1632 | val loss: 1.1761 | val acc: 0.5463\n",
            "Training epoch 347\n",
            "train loss: 1.1613 | val loss: 1.1762 | val acc: 0.5406\n",
            "Training epoch 348\n",
            "train loss: 1.1593 | val loss: 1.1749 | val acc: 0.5463\n",
            "Training epoch 349\n",
            "train loss: 1.1596 | val loss: 1.1745 | val acc: 0.5442\n",
            "Training epoch 350\n",
            "train loss: 1.1613 | val loss: 1.1742 | val acc: 0.5449\n",
            "Training epoch 351\n",
            "train loss: 1.1559 | val loss: 1.1744 | val acc: 0.5456\n",
            "Training epoch 352\n",
            "train loss: 1.1597 | val loss: 1.1731 | val acc: 0.5456\n",
            "Training epoch 353\n",
            "train loss: 1.1611 | val loss: 1.1732 | val acc: 0.5477\n",
            "Training epoch 354\n",
            "train loss: 1.1616 | val loss: 1.1739 | val acc: 0.5477\n",
            "Training epoch 355\n",
            "train loss: 1.1554 | val loss: 1.1728 | val acc: 0.5449\n",
            "Training epoch 356\n",
            "train loss: 1.1520 | val loss: 1.1719 | val acc: 0.5484\n",
            "Training epoch 357\n",
            "train loss: 1.1581 | val loss: 1.1724 | val acc: 0.5420\n",
            "Training epoch 358\n",
            "train loss: 1.1545 | val loss: 1.1728 | val acc: 0.5456\n",
            "Training epoch 359\n",
            "train loss: 1.1500 | val loss: 1.1728 | val acc: 0.5442\n",
            "Training epoch 360\n",
            "train loss: 1.1523 | val loss: 1.1702 | val acc: 0.5477\n",
            "Training epoch 361\n",
            "train loss: 1.1499 | val loss: 1.1706 | val acc: 0.5484\n",
            "Training epoch 362\n",
            "train loss: 1.1596 | val loss: 1.1702 | val acc: 0.5456\n",
            "Training epoch 363\n",
            "train loss: 1.1518 | val loss: 1.1700 | val acc: 0.5470\n",
            "Training epoch 364\n",
            "train loss: 1.1452 | val loss: 1.1688 | val acc: 0.5456\n",
            "Training epoch 365\n",
            "train loss: 1.1543 | val loss: 1.1687 | val acc: 0.5420\n",
            "Training epoch 366\n",
            "train loss: 1.1575 | val loss: 1.1685 | val acc: 0.5491\n",
            "Training epoch 367\n",
            "train loss: 1.1488 | val loss: 1.1681 | val acc: 0.5499\n",
            "Training epoch 368\n",
            "train loss: 1.1479 | val loss: 1.1670 | val acc: 0.5491\n",
            "Training epoch 369\n",
            "train loss: 1.1520 | val loss: 1.1671 | val acc: 0.5456\n",
            "Training epoch 370\n",
            "train loss: 1.1497 | val loss: 1.1662 | val acc: 0.5449\n",
            "Training epoch 371\n",
            "train loss: 1.1476 | val loss: 1.1660 | val acc: 0.5470\n",
            "Training epoch 372\n",
            "train loss: 1.1454 | val loss: 1.1657 | val acc: 0.5477\n",
            "Training epoch 373\n",
            "train loss: 1.1493 | val loss: 1.1649 | val acc: 0.5484\n",
            "Training epoch 374\n",
            "train loss: 1.1490 | val loss: 1.1649 | val acc: 0.5427\n",
            "Training epoch 375\n",
            "train loss: 1.1446 | val loss: 1.1654 | val acc: 0.5477\n",
            "Training epoch 376\n",
            "train loss: 1.1479 | val loss: 1.1660 | val acc: 0.5463\n",
            "Training epoch 377\n",
            "train loss: 1.1425 | val loss: 1.1655 | val acc: 0.5484\n",
            "Training epoch 378\n",
            "train loss: 1.1436 | val loss: 1.1664 | val acc: 0.5434\n",
            "Training epoch 379\n",
            "train loss: 1.1435 | val loss: 1.1661 | val acc: 0.5470\n",
            "Training epoch 380\n",
            "train loss: 1.1424 | val loss: 1.1625 | val acc: 0.5477\n",
            "Training epoch 381\n",
            "train loss: 1.1425 | val loss: 1.1615 | val acc: 0.5491\n",
            "Training epoch 382\n",
            "train loss: 1.1441 | val loss: 1.1629 | val acc: 0.5463\n",
            "Training epoch 383\n",
            "train loss: 1.1456 | val loss: 1.1618 | val acc: 0.5477\n",
            "Training epoch 384\n",
            "train loss: 1.1429 | val loss: 1.1616 | val acc: 0.5520\n",
            "Training epoch 385\n",
            "train loss: 1.1405 | val loss: 1.1615 | val acc: 0.5477\n",
            "Training epoch 386\n",
            "train loss: 1.1405 | val loss: 1.1610 | val acc: 0.5484\n",
            "Training epoch 387\n",
            "train loss: 1.1401 | val loss: 1.1608 | val acc: 0.5484\n",
            "Training epoch 388\n",
            "train loss: 1.1405 | val loss: 1.1594 | val acc: 0.5477\n",
            "Training epoch 389\n",
            "train loss: 1.1407 | val loss: 1.1599 | val acc: 0.5477\n",
            "Training epoch 390\n",
            "train loss: 1.1367 | val loss: 1.1596 | val acc: 0.5499\n",
            "Training epoch 391\n",
            "train loss: 1.1370 | val loss: 1.1586 | val acc: 0.5484\n",
            "Training epoch 392\n",
            "train loss: 1.1385 | val loss: 1.1589 | val acc: 0.5477\n",
            "Training epoch 393\n",
            "train loss: 1.1349 | val loss: 1.1578 | val acc: 0.5484\n",
            "Training epoch 394\n",
            "train loss: 1.1371 | val loss: 1.1575 | val acc: 0.5506\n",
            "Training epoch 395\n",
            "train loss: 1.1324 | val loss: 1.1571 | val acc: 0.5477\n",
            "Training epoch 396\n",
            "train loss: 1.1392 | val loss: 1.1578 | val acc: 0.5527\n",
            "Training epoch 397\n",
            "train loss: 1.1362 | val loss: 1.1567 | val acc: 0.5556\n",
            "Training epoch 398\n",
            "train loss: 1.1337 | val loss: 1.1557 | val acc: 0.5520\n",
            "Training epoch 399\n",
            "train loss: 1.1356 | val loss: 1.1561 | val acc: 0.5484\n",
            "Training epoch 400\n",
            "train loss: 1.1325 | val loss: 1.1557 | val acc: 0.5463\n",
            "Training epoch 401\n",
            "train loss: 1.1320 | val loss: 1.1551 | val acc: 0.5499\n",
            "Training epoch 402\n",
            "train loss: 1.1314 | val loss: 1.1546 | val acc: 0.5491\n",
            "Training epoch 403\n",
            "train loss: 1.1345 | val loss: 1.1544 | val acc: 0.5463\n",
            "Training epoch 404\n",
            "train loss: 1.1327 | val loss: 1.1544 | val acc: 0.5513\n",
            "Training epoch 405\n",
            "train loss: 1.1300 | val loss: 1.1535 | val acc: 0.5513\n",
            "Training epoch 406\n",
            "train loss: 1.1327 | val loss: 1.1541 | val acc: 0.5484\n",
            "Training epoch 407\n",
            "train loss: 1.1280 | val loss: 1.1523 | val acc: 0.5499\n",
            "Training epoch 408\n",
            "train loss: 1.1312 | val loss: 1.1523 | val acc: 0.5484\n",
            "Training epoch 409\n",
            "train loss: 1.1236 | val loss: 1.1519 | val acc: 0.5534\n",
            "Training epoch 410\n",
            "train loss: 1.1304 | val loss: 1.1523 | val acc: 0.5541\n",
            "Training epoch 411\n",
            "train loss: 1.1276 | val loss: 1.1522 | val acc: 0.5577\n",
            "Training epoch 412\n",
            "train loss: 1.1266 | val loss: 1.1514 | val acc: 0.5534\n",
            "Training epoch 413\n",
            "train loss: 1.1264 | val loss: 1.1512 | val acc: 0.5520\n",
            "Training epoch 414\n",
            "train loss: 1.1280 | val loss: 1.1500 | val acc: 0.5477\n",
            "Training epoch 415\n",
            "train loss: 1.1270 | val loss: 1.1506 | val acc: 0.5513\n",
            "Training epoch 416\n",
            "train loss: 1.1269 | val loss: 1.1512 | val acc: 0.5484\n",
            "Training epoch 417\n",
            "train loss: 1.1264 | val loss: 1.1491 | val acc: 0.5463\n",
            "Training epoch 418\n",
            "train loss: 1.1237 | val loss: 1.1497 | val acc: 0.5470\n",
            "Training epoch 419\n",
            "train loss: 1.1269 | val loss: 1.1489 | val acc: 0.5491\n",
            "Training epoch 420\n",
            "train loss: 1.1260 | val loss: 1.1491 | val acc: 0.5570\n",
            "Training epoch 421\n",
            "train loss: 1.1275 | val loss: 1.1476 | val acc: 0.5477\n",
            "Training epoch 422\n",
            "train loss: 1.1262 | val loss: 1.1472 | val acc: 0.5491\n",
            "Training epoch 423\n",
            "train loss: 1.1227 | val loss: 1.1468 | val acc: 0.5470\n",
            "Training epoch 424\n",
            "train loss: 1.1229 | val loss: 1.1463 | val acc: 0.5506\n",
            "Training epoch 425\n",
            "train loss: 1.1225 | val loss: 1.1471 | val acc: 0.5513\n",
            "Training epoch 426\n",
            "train loss: 1.1210 | val loss: 1.1472 | val acc: 0.5527\n",
            "Training epoch 427\n",
            "train loss: 1.1202 | val loss: 1.1453 | val acc: 0.5506\n",
            "Training epoch 428\n",
            "train loss: 1.1228 | val loss: 1.1464 | val acc: 0.5527\n",
            "Training epoch 429\n",
            "train loss: 1.1221 | val loss: 1.1443 | val acc: 0.5491\n",
            "Training epoch 430\n",
            "train loss: 1.1247 | val loss: 1.1444 | val acc: 0.5470\n",
            "Training epoch 431\n",
            "train loss: 1.1199 | val loss: 1.1446 | val acc: 0.5463\n",
            "Training epoch 432\n",
            "train loss: 1.1185 | val loss: 1.1448 | val acc: 0.5499\n",
            "Training epoch 433\n",
            "train loss: 1.1205 | val loss: 1.1437 | val acc: 0.5491\n",
            "Training epoch 434\n",
            "train loss: 1.1185 | val loss: 1.1437 | val acc: 0.5520\n",
            "Training epoch 435\n",
            "train loss: 1.1202 | val loss: 1.1426 | val acc: 0.5499\n",
            "Training epoch 436\n",
            "train loss: 1.1143 | val loss: 1.1420 | val acc: 0.5484\n",
            "Training epoch 437\n",
            "train loss: 1.1123 | val loss: 1.1430 | val acc: 0.5513\n",
            "Training epoch 438\n",
            "train loss: 1.1187 | val loss: 1.1418 | val acc: 0.5513\n",
            "Training epoch 439\n",
            "train loss: 1.1116 | val loss: 1.1421 | val acc: 0.5499\n",
            "Training epoch 440\n",
            "train loss: 1.1188 | val loss: 1.1414 | val acc: 0.5463\n",
            "Training epoch 441\n",
            "train loss: 1.1146 | val loss: 1.1416 | val acc: 0.5527\n",
            "Training epoch 442\n",
            "train loss: 1.1097 | val loss: 1.1411 | val acc: 0.5506\n",
            "Training epoch 443\n",
            "train loss: 1.1139 | val loss: 1.1406 | val acc: 0.5491\n",
            "Training epoch 444\n",
            "train loss: 1.1136 | val loss: 1.1406 | val acc: 0.5520\n",
            "Training epoch 445\n",
            "train loss: 1.1134 | val loss: 1.1410 | val acc: 0.5527\n",
            "Training epoch 446\n",
            "train loss: 1.1168 | val loss: 1.1403 | val acc: 0.5477\n",
            "Training epoch 447\n",
            "train loss: 1.1115 | val loss: 1.1404 | val acc: 0.5527\n",
            "Training epoch 448\n",
            "train loss: 1.1110 | val loss: 1.1404 | val acc: 0.5520\n",
            "Training epoch 449\n",
            "train loss: 1.1142 | val loss: 1.1391 | val acc: 0.5548\n",
            "Training epoch 450\n",
            "train loss: 1.1096 | val loss: 1.1388 | val acc: 0.5577\n",
            "Training epoch 451\n",
            "train loss: 1.1040 | val loss: 1.1373 | val acc: 0.5534\n",
            "Training epoch 452\n",
            "train loss: 1.1096 | val loss: 1.1371 | val acc: 0.5527\n",
            "Training epoch 453\n",
            "train loss: 1.1099 | val loss: 1.1373 | val acc: 0.5513\n",
            "Training epoch 454\n",
            "train loss: 1.1114 | val loss: 1.1371 | val acc: 0.5520\n",
            "Training epoch 455\n",
            "train loss: 1.1075 | val loss: 1.1372 | val acc: 0.5534\n",
            "Training epoch 456\n",
            "train loss: 1.1120 | val loss: 1.1381 | val acc: 0.5577\n",
            "Training epoch 457\n",
            "train loss: 1.1124 | val loss: 1.1364 | val acc: 0.5598\n",
            "Training epoch 458\n",
            "train loss: 1.1108 | val loss: 1.1356 | val acc: 0.5541\n",
            "Training epoch 459\n",
            "train loss: 1.1046 | val loss: 1.1355 | val acc: 0.5534\n",
            "Training epoch 460\n",
            "train loss: 1.1005 | val loss: 1.1355 | val acc: 0.5548\n",
            "Training epoch 461\n",
            "train loss: 1.1071 | val loss: 1.1355 | val acc: 0.5584\n",
            "Training epoch 462\n",
            "train loss: 1.1075 | val loss: 1.1361 | val acc: 0.5541\n",
            "Training epoch 463\n",
            "train loss: 1.1102 | val loss: 1.1360 | val acc: 0.5563\n",
            "Training epoch 464\n",
            "train loss: 1.1076 | val loss: 1.1338 | val acc: 0.5570\n",
            "Training epoch 465\n",
            "train loss: 1.1046 | val loss: 1.1341 | val acc: 0.5591\n",
            "Training epoch 466\n",
            "train loss: 1.1037 | val loss: 1.1334 | val acc: 0.5563\n",
            "Training epoch 467\n",
            "train loss: 1.1002 | val loss: 1.1331 | val acc: 0.5591\n",
            "Training epoch 468\n",
            "train loss: 1.1055 | val loss: 1.1330 | val acc: 0.5563\n",
            "Training epoch 469\n",
            "train loss: 1.1039 | val loss: 1.1323 | val acc: 0.5591\n",
            "Training epoch 470\n",
            "train loss: 1.1054 | val loss: 1.1319 | val acc: 0.5563\n",
            "Training epoch 471\n",
            "train loss: 1.0982 | val loss: 1.1314 | val acc: 0.5563\n",
            "Training epoch 472\n",
            "train loss: 1.1022 | val loss: 1.1315 | val acc: 0.5570\n",
            "Training epoch 473\n",
            "train loss: 1.1001 | val loss: 1.1313 | val acc: 0.5620\n",
            "Training epoch 474\n",
            "train loss: 1.0979 | val loss: 1.1305 | val acc: 0.5598\n",
            "Training epoch 475\n",
            "train loss: 1.0996 | val loss: 1.1298 | val acc: 0.5577\n",
            "Training epoch 476\n",
            "train loss: 1.1012 | val loss: 1.1300 | val acc: 0.5584\n",
            "Training epoch 477\n",
            "train loss: 1.0973 | val loss: 1.1294 | val acc: 0.5591\n",
            "Training epoch 478\n",
            "train loss: 1.0984 | val loss: 1.1302 | val acc: 0.5548\n",
            "Training epoch 479\n",
            "train loss: 1.0954 | val loss: 1.1294 | val acc: 0.5556\n",
            "Training epoch 480\n",
            "train loss: 1.1031 | val loss: 1.1304 | val acc: 0.5634\n",
            "Training epoch 481\n",
            "train loss: 1.0965 | val loss: 1.1297 | val acc: 0.5556\n",
            "Training epoch 482\n",
            "train loss: 1.0937 | val loss: 1.1284 | val acc: 0.5648\n",
            "Training epoch 483\n",
            "train loss: 1.0964 | val loss: 1.1299 | val acc: 0.5570\n",
            "Training epoch 484\n",
            "train loss: 1.0970 | val loss: 1.1286 | val acc: 0.5584\n",
            "Training epoch 485\n",
            "train loss: 1.0925 | val loss: 1.1269 | val acc: 0.5605\n",
            "Training epoch 486\n",
            "train loss: 1.0932 | val loss: 1.1274 | val acc: 0.5662\n",
            "Training epoch 487\n",
            "train loss: 1.0925 | val loss: 1.1267 | val acc: 0.5605\n",
            "Training epoch 488\n",
            "train loss: 1.0981 | val loss: 1.1278 | val acc: 0.5591\n",
            "Training epoch 489\n",
            "train loss: 1.0940 | val loss: 1.1265 | val acc: 0.5620\n",
            "Training epoch 490\n",
            "train loss: 1.0918 | val loss: 1.1259 | val acc: 0.5605\n",
            "Training epoch 491\n",
            "train loss: 1.0952 | val loss: 1.1261 | val acc: 0.5613\n",
            "Training epoch 492\n",
            "train loss: 1.0901 | val loss: 1.1254 | val acc: 0.5627\n",
            "Training epoch 493\n",
            "train loss: 1.0878 | val loss: 1.1270 | val acc: 0.5655\n",
            "Training epoch 494\n",
            "train loss: 1.0890 | val loss: 1.1243 | val acc: 0.5620\n",
            "Training epoch 495\n",
            "train loss: 1.0943 | val loss: 1.1249 | val acc: 0.5620\n",
            "Training epoch 496\n",
            "train loss: 1.0916 | val loss: 1.1244 | val acc: 0.5634\n",
            "Training epoch 497\n",
            "train loss: 1.0941 | val loss: 1.1242 | val acc: 0.5641\n",
            "Training epoch 498\n",
            "train loss: 1.0914 | val loss: 1.1234 | val acc: 0.5605\n",
            "Training epoch 499\n",
            "train loss: 1.0876 | val loss: 1.1238 | val acc: 0.5613\n",
            "Training epoch 500\n",
            "train loss: 1.0864 | val loss: 1.1234 | val acc: 0.5605\n",
            "Training epoch 501\n",
            "train loss: 1.0860 | val loss: 1.1226 | val acc: 0.5605\n",
            "Training epoch 502\n",
            "train loss: 1.0897 | val loss: 1.1247 | val acc: 0.5584\n",
            "Training epoch 503\n",
            "train loss: 1.0844 | val loss: 1.1226 | val acc: 0.5655\n",
            "Training epoch 504\n",
            "train loss: 1.0895 | val loss: 1.1216 | val acc: 0.5613\n",
            "Training epoch 505\n",
            "train loss: 1.0839 | val loss: 1.1219 | val acc: 0.5705\n",
            "Training epoch 506\n",
            "train loss: 1.0857 | val loss: 1.1211 | val acc: 0.5620\n",
            "Training epoch 507\n",
            "train loss: 1.0874 | val loss: 1.1222 | val acc: 0.5627\n",
            "Training epoch 508\n",
            "train loss: 1.0876 | val loss: 1.1215 | val acc: 0.5698\n",
            "Training epoch 509\n",
            "train loss: 1.0886 | val loss: 1.1212 | val acc: 0.5648\n",
            "Training epoch 510\n",
            "train loss: 1.0849 | val loss: 1.1193 | val acc: 0.5698\n",
            "Training epoch 511\n",
            "train loss: 1.0834 | val loss: 1.1209 | val acc: 0.5677\n",
            "Training epoch 512\n",
            "train loss: 1.0791 | val loss: 1.1200 | val acc: 0.5648\n",
            "Training epoch 513\n",
            "train loss: 1.0842 | val loss: 1.1207 | val acc: 0.5605\n",
            "Training epoch 514\n",
            "train loss: 1.0797 | val loss: 1.1190 | val acc: 0.5684\n",
            "Training epoch 515\n",
            "train loss: 1.0815 | val loss: 1.1195 | val acc: 0.5613\n",
            "Training epoch 516\n",
            "train loss: 1.0837 | val loss: 1.1188 | val acc: 0.5648\n",
            "Training epoch 517\n",
            "train loss: 1.0792 | val loss: 1.1189 | val acc: 0.5655\n",
            "Training epoch 518\n",
            "train loss: 1.0799 | val loss: 1.1192 | val acc: 0.5620\n",
            "Training epoch 519\n",
            "train loss: 1.0810 | val loss: 1.1175 | val acc: 0.5662\n",
            "Training epoch 520\n",
            "train loss: 1.0787 | val loss: 1.1176 | val acc: 0.5662\n",
            "Training epoch 521\n",
            "train loss: 1.0773 | val loss: 1.1182 | val acc: 0.5691\n",
            "Training epoch 522\n",
            "train loss: 1.0788 | val loss: 1.1174 | val acc: 0.5655\n",
            "Training epoch 523\n",
            "train loss: 1.0826 | val loss: 1.1160 | val acc: 0.5655\n",
            "Training epoch 524\n",
            "train loss: 1.0752 | val loss: 1.1165 | val acc: 0.5641\n",
            "Training epoch 525\n",
            "train loss: 1.0788 | val loss: 1.1168 | val acc: 0.5662\n",
            "Training epoch 526\n",
            "train loss: 1.0787 | val loss: 1.1174 | val acc: 0.5662\n",
            "Training epoch 527\n",
            "train loss: 1.0779 | val loss: 1.1154 | val acc: 0.5641\n",
            "Training epoch 528\n",
            "train loss: 1.0806 | val loss: 1.1173 | val acc: 0.5705\n",
            "Training epoch 529\n",
            "train loss: 1.0758 | val loss: 1.1180 | val acc: 0.5584\n",
            "Training epoch 530\n",
            "train loss: 1.0784 | val loss: 1.1146 | val acc: 0.5648\n",
            "Training epoch 531\n",
            "train loss: 1.0816 | val loss: 1.1159 | val acc: 0.5734\n",
            "Training epoch 532\n",
            "train loss: 1.0813 | val loss: 1.1150 | val acc: 0.5719\n",
            "Training epoch 533\n",
            "train loss: 1.0679 | val loss: 1.1145 | val acc: 0.5662\n",
            "Training epoch 534\n",
            "train loss: 1.0768 | val loss: 1.1133 | val acc: 0.5705\n",
            "Training epoch 535\n",
            "train loss: 1.0724 | val loss: 1.1152 | val acc: 0.5677\n",
            "Training epoch 536\n",
            "train loss: 1.0772 | val loss: 1.1143 | val acc: 0.5648\n",
            "Training epoch 537\n",
            "train loss: 1.0718 | val loss: 1.1149 | val acc: 0.5670\n",
            "Training epoch 538\n",
            "train loss: 1.0700 | val loss: 1.1141 | val acc: 0.5791\n",
            "Training epoch 539\n",
            "train loss: 1.0719 | val loss: 1.1124 | val acc: 0.5691\n",
            "Training epoch 540\n",
            "train loss: 1.0718 | val loss: 1.1129 | val acc: 0.5662\n",
            "Training epoch 541\n",
            "train loss: 1.0705 | val loss: 1.1127 | val acc: 0.5691\n",
            "Training epoch 542\n",
            "train loss: 1.0738 | val loss: 1.1128 | val acc: 0.5712\n",
            "Training epoch 543\n",
            "train loss: 1.0652 | val loss: 1.1135 | val acc: 0.5741\n",
            "Training epoch 544\n",
            "train loss: 1.0714 | val loss: 1.1111 | val acc: 0.5662\n",
            "Training epoch 545\n",
            "train loss: 1.0724 | val loss: 1.1116 | val acc: 0.5734\n",
            "Training epoch 546\n",
            "train loss: 1.0695 | val loss: 1.1118 | val acc: 0.5684\n",
            "Training epoch 547\n",
            "train loss: 1.0689 | val loss: 1.1097 | val acc: 0.5641\n",
            "Training epoch 548\n",
            "train loss: 1.0700 | val loss: 1.1117 | val acc: 0.5776\n",
            "Training epoch 549\n",
            "train loss: 1.0747 | val loss: 1.1114 | val acc: 0.5719\n",
            "Training epoch 550\n",
            "train loss: 1.0652 | val loss: 1.1095 | val acc: 0.5726\n",
            "Training epoch 551\n",
            "train loss: 1.0738 | val loss: 1.1112 | val acc: 0.5791\n",
            "Training epoch 552\n",
            "train loss: 1.0682 | val loss: 1.1095 | val acc: 0.5741\n",
            "Training epoch 553\n",
            "train loss: 1.0693 | val loss: 1.1090 | val acc: 0.5691\n",
            "Training epoch 554\n",
            "train loss: 1.0677 | val loss: 1.1102 | val acc: 0.5655\n",
            "Training epoch 555\n",
            "train loss: 1.0644 | val loss: 1.1098 | val acc: 0.5719\n",
            "Training epoch 556\n",
            "train loss: 1.0632 | val loss: 1.1113 | val acc: 0.5634\n",
            "Training epoch 557\n",
            "train loss: 1.0690 | val loss: 1.1093 | val acc: 0.5741\n",
            "Training epoch 558\n",
            "train loss: 1.0665 | val loss: 1.1091 | val acc: 0.5677\n",
            "Training epoch 559\n",
            "train loss: 1.0639 | val loss: 1.1083 | val acc: 0.5741\n",
            "Training epoch 560\n",
            "train loss: 1.0704 | val loss: 1.1077 | val acc: 0.5762\n",
            "Training epoch 561\n",
            "train loss: 1.0617 | val loss: 1.1086 | val acc: 0.5698\n",
            "Training epoch 562\n",
            "train loss: 1.0679 | val loss: 1.1082 | val acc: 0.5691\n",
            "Training epoch 563\n",
            "train loss: 1.0611 | val loss: 1.1072 | val acc: 0.5698\n",
            "Training epoch 564\n",
            "train loss: 1.0634 | val loss: 1.1065 | val acc: 0.5670\n",
            "Training epoch 565\n",
            "train loss: 1.0644 | val loss: 1.1071 | val acc: 0.5691\n",
            "Training epoch 566\n",
            "train loss: 1.0602 | val loss: 1.1073 | val acc: 0.5627\n",
            "Training epoch 567\n",
            "train loss: 1.0623 | val loss: 1.1061 | val acc: 0.5712\n",
            "Training epoch 568\n",
            "train loss: 1.0675 | val loss: 1.1061 | val acc: 0.5726\n",
            "Training epoch 569\n",
            "train loss: 1.0621 | val loss: 1.1053 | val acc: 0.5705\n",
            "Training epoch 570\n",
            "train loss: 1.0596 | val loss: 1.1062 | val acc: 0.5698\n",
            "Training epoch 571\n",
            "train loss: 1.0597 | val loss: 1.1058 | val acc: 0.5798\n",
            "Training epoch 572\n",
            "train loss: 1.0570 | val loss: 1.1046 | val acc: 0.5769\n",
            "Training epoch 573\n",
            "train loss: 1.0579 | val loss: 1.1041 | val acc: 0.5734\n",
            "Training epoch 574\n",
            "train loss: 1.0578 | val loss: 1.1034 | val acc: 0.5748\n",
            "Training epoch 575\n",
            "train loss: 1.0590 | val loss: 1.1040 | val acc: 0.5726\n",
            "Training epoch 576\n",
            "train loss: 1.0548 | val loss: 1.1053 | val acc: 0.5734\n",
            "Training epoch 577\n",
            "train loss: 1.0557 | val loss: 1.1034 | val acc: 0.5719\n",
            "Training epoch 578\n",
            "train loss: 1.0599 | val loss: 1.1051 | val acc: 0.5762\n",
            "Training epoch 579\n",
            "train loss: 1.0563 | val loss: 1.1038 | val acc: 0.5719\n",
            "Training epoch 580\n",
            "train loss: 1.0556 | val loss: 1.1043 | val acc: 0.5741\n",
            "Training epoch 581\n",
            "train loss: 1.0562 | val loss: 1.1039 | val acc: 0.5698\n",
            "Training epoch 582\n",
            "train loss: 1.0515 | val loss: 1.1038 | val acc: 0.5719\n",
            "Training epoch 583\n",
            "train loss: 1.0511 | val loss: 1.1021 | val acc: 0.5748\n",
            "Training epoch 584\n",
            "train loss: 1.0561 | val loss: 1.1027 | val acc: 0.5705\n",
            "Training epoch 585\n",
            "train loss: 1.0573 | val loss: 1.1031 | val acc: 0.5712\n",
            "Training epoch 586\n",
            "train loss: 1.0514 | val loss: 1.1024 | val acc: 0.5755\n",
            "Training epoch 587\n",
            "train loss: 1.0539 | val loss: 1.1021 | val acc: 0.5726\n",
            "Training epoch 588\n",
            "train loss: 1.0513 | val loss: 1.1027 | val acc: 0.5712\n",
            "Training epoch 589\n",
            "train loss: 1.0521 | val loss: 1.1012 | val acc: 0.5726\n",
            "Training epoch 590\n",
            "train loss: 1.0516 | val loss: 1.1025 | val acc: 0.5698\n",
            "Training epoch 591\n",
            "train loss: 1.0541 | val loss: 1.1018 | val acc: 0.5769\n",
            "Training epoch 592\n",
            "train loss: 1.0520 | val loss: 1.1019 | val acc: 0.5748\n",
            "Training epoch 593\n",
            "train loss: 1.0466 | val loss: 1.1006 | val acc: 0.5712\n",
            "Training epoch 594\n",
            "train loss: 1.0558 | val loss: 1.0999 | val acc: 0.5741\n",
            "Training epoch 595\n",
            "train loss: 1.0467 | val loss: 1.0999 | val acc: 0.5783\n",
            "Training epoch 596\n",
            "train loss: 1.0476 | val loss: 1.0990 | val acc: 0.5741\n",
            "Training epoch 597\n",
            "train loss: 1.0553 | val loss: 1.0989 | val acc: 0.5741\n",
            "Training epoch 598\n",
            "train loss: 1.0513 | val loss: 1.0986 | val acc: 0.5726\n",
            "Training epoch 599\n",
            "train loss: 1.0477 | val loss: 1.1003 | val acc: 0.5755\n",
            "Training epoch 600\n",
            "train loss: 1.0509 | val loss: 1.0991 | val acc: 0.5734\n",
            "Training took 18 minutes and 38 seconds\n",
            "Final test acc: 0.5773\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('model',\n",
        "    #                     choices=['logistic_regression', 'mlp'],\n",
        "    #                     default='logistic_regression',\n",
        "    #                     help=\"Which model should the script run?\")\n",
        "    # parser.add_argument('-epochs', default=200, type=int,\n",
        "    #                     help=\"\"\"Number of epochs to train for. You should not\n",
        "    #                     need to change this value for your plots.\"\"\")\n",
        "    # parser.add_argument('-batch_size', default=64, type=int,\n",
        "    #                     help=\"Size of training batch.\")\n",
        "    # parser.add_argument('-hidden_size', type=int, default=200)\n",
        "    # parser.add_argument('-layers', type=int, default=2)\n",
        "    # parser.add_argument('-learning_rate', type=float, default=0.002)\n",
        "    # parser.add_argument('-l2_decay', type=float, default=0.0)\n",
        "    # parser.add_argument('-dropout', type=float, default=0.3)\n",
        "    # parser.add_argument('-momentum', type=float, default=0.0)\n",
        "    # parser.add_argument('-activation',\n",
        "    #                     choices=['tanh', 'relu'], default='relu')\n",
        "    # parser.add_argument('-optimizer',\n",
        "    #                     choices=['sgd', 'adam'], default='sgd')\n",
        "    # parser.add_argument('-data_path', type=str, default='intel_landscapes.npz',)\n",
        "\n",
        "    #opt = parser.parse_args()\n",
        "    opt = {\n",
        "        'model': 'mlp',\n",
        "        'epochs': 600,\n",
        "        'batch_size': 512,\n",
        "        'hidden_size': 200,\n",
        "        'layers': 2,\n",
        "        'learning_rate': 0.002,\n",
        "        'l2_decay': 0.0,\n",
        "        'dropout': 0.3,\n",
        "        'momentum': 0.0,\n",
        "        'activation': 'relu',\n",
        "        'optimizer': 'sgd',\n",
        "        'data_path': '/content/sample_data/intel_landscapes.v2.npz',\n",
        "    }\n",
        "\n",
        "    configure_seed(seed=42)\n",
        "\n",
        "    data = load_dataset(opt['data_path'])\n",
        "    dataset = ClassificationDataset(data)\n",
        "    train_dataloader = DataLoader(\n",
        "        dataset, batch_size=opt['batch_size'], shuffle=True, generator=torch.Generator().manual_seed(42))\n",
        "    dev_X, dev_y = dataset.dev_X, dataset.dev_y\n",
        "    test_X, test_y = dataset.test_X, dataset.test_y\n",
        "\n",
        "    n_classes = torch.unique(dataset.y).shape[0]  # 10\n",
        "    n_feats = dataset.X.shape[1]\n",
        "\n",
        "    # initialize the model\n",
        "    if opt['model'] == 'logistic_regression':\n",
        "        model = LogisticRegression(n_classes, n_feats)\n",
        "    else:\n",
        "        model = FeedforwardNetwork(\n",
        "            n_classes,\n",
        "            n_feats,\n",
        "            opt['hidden_size'],\n",
        "            opt['layers'],\n",
        "            opt['activation'],\n",
        "            opt['dropout']\n",
        "        )\n",
        "\n",
        "    # get an optimizer\n",
        "    optims = {\"adam\": torch.optim.Adam, \"sgd\": torch.optim.SGD}\n",
        "\n",
        "    optim_cls = optims[opt['optimizer']]\n",
        "    optimizer = optim_cls(\n",
        "        model.parameters(), lr=opt['learning_rate'], weight_decay=opt['l2_decay'], momentum = opt['momentum']\n",
        "    )\n",
        "\n",
        "    # get a loss criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # training loop\n",
        "    epochs = torch.arange(1, opt['epochs'] + 1)\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    print('initial val acc: {:.4f}'.format(evaluate(model, dev_X, dev_y, criterion)[1]))\n",
        "\n",
        "    for ii in epochs:\n",
        "        print('Training epoch {}'.format(ii))\n",
        "        epoch_train_losses = []\n",
        "        for X_batch, y_batch in train_dataloader:\n",
        "            loss = train_batch(\n",
        "                X_batch, y_batch, model, optimizer, criterion)\n",
        "            epoch_train_losses.append(loss)\n",
        "\n",
        "        epoch_train_loss = torch.tensor(epoch_train_losses).mean().item()\n",
        "        val_loss, val_acc = evaluate(model, dev_X, dev_y, criterion)\n",
        "\n",
        "        print('train loss: {:.4f} | val loss: {:.4f} | val acc: {:.4f}'.format(\n",
        "            epoch_train_loss, val_loss, val_acc\n",
        "        ))\n",
        "\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        valid_losses.append(val_loss)\n",
        "        valid_accs.append(val_acc)\n",
        "\n",
        "    elapsed_time = time.time() - start\n",
        "    minutes = int(elapsed_time // 60)\n",
        "    seconds = int(elapsed_time % 60)\n",
        "    print('Training took {} minutes and {} seconds'.format(minutes, seconds))\n",
        "\n",
        "    _, test_acc = evaluate(model, test_X, test_y, criterion)\n",
        "    print('Final test acc: {:.4f}'.format(test_acc))\n",
        "\n",
        "    # plot\n",
        "    if opt['model'] == \"logistic_regression\":\n",
        "        config = (\n",
        "            f\"batch-{opt['batch_size']}-lr-{opt['learning_rate']}-epochs-{opt['epochs']}-\"\n",
        "            f\"l2-{opt['l2_decay']}-opt-{opt['optimizer']}\"\n",
        "        )\n",
        "    else:\n",
        "        config = (\n",
        "            f\"batch-{opt['batch_size']}-lr-{opt['learning_rate']}-epochs-{opt['epochs']}-\"\n",
        "            f\"hidden-{opt['hidden_size']}-dropout-{opt['dropout']}-l2-{opt['l2_decay']}-\"\n",
        "            f\"layers-{opt['layers']}-act-{opt['activation']}-opt-{opt['optimizer']}-mom-{opt['momentum']}\"\n",
        "        )\n",
        "\n",
        "    losses = {\n",
        "        \"Train Loss\": train_losses,\n",
        "        \"Valid Loss\": valid_losses,\n",
        "    }\n",
        "\n",
        "\n",
        "    plot(epochs, losses, filename=f\"{opt['model']}-training-loss-{config}.pdf\")\n",
        "    #accuracy = { \"Valid Accuracy\": valid_accs }\n",
        "    #plot(epochs, accuracy, filename=f\"{opt['model']}-validation-accuracy-{config}.pdf\")\n",
        "    # Plot the results\n",
        "    #plot_results(epochs, train_losses, valid_losses, valid_accs, config, opt)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}